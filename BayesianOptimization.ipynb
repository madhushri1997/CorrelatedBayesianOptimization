{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFZF5YgTfDy5",
        "outputId": "52211788-b3fe-4d31-bae9-f42e446c0638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting botorch\n",
            "  Downloading botorch-0.8.0-py3-none-any.whl (481 kB)\n",
            "\u001b[K     |████████████████████████████████| 481 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting gpytorch==1.9.0\n",
            "  Downloading gpytorch-1.9.0-py3-none-any.whl (245 kB)\n",
            "\u001b[K     |████████████████████████████████| 245 kB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from botorch) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from botorch) (1.13.0+cu116)\n",
            "Collecting pyro-ppl>=1.8.2\n",
            "  Downloading pyro_ppl-1.8.3-py3-none-any.whl (727 kB)\n",
            "\u001b[K     |████████████████████████████████| 727 kB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multipledispatch in /usr/local/lib/python3.8/dist-packages (from botorch) (0.6.0)\n",
            "Collecting linear-operator==0.2.0\n",
            "  Downloading linear_operator-0.2.0-py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 25.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from gpytorch==1.9.0->botorch) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.8/dist-packages (from pyro-ppl>=1.8.2->botorch) (4.64.1)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.8/dist-packages (from pyro-ppl>=1.8.2->botorch) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from pyro-ppl>=1.8.2->botorch) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->botorch) (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from multipledispatch->botorch) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch==1.9.0->botorch) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch==1.9.0->botorch) (1.2.0)\n",
            "Installing collected packages: pyro-api, linear-operator, pyro-ppl, gpytorch, botorch\n",
            "Successfully installed botorch-0.8.0 gpytorch-1.9.0 linear-operator-0.2.0 pyro-api-0.1.2 pyro-ppl-1.8.3\n"
          ]
        }
      ],
      "source": [
        "!pip install botorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd6gMydbfGWa",
        "outputId": "1174c517-ec13-4c6f-b837-fb3b16805d50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpytorch in /usr/local/lib/python3.8/dist-packages (1.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from gpytorch) (1.0.2)\n",
            "Requirement already satisfied: linear-operator>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from gpytorch) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from linear-operator>=0.1.1->gpytorch) (1.13.0+cu116)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from linear-operator>=0.1.1->gpytorch) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->linear-operator>=0.1.1->gpytorch) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/cornellius-gp/gpytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPPC7V7TfJaW",
        "outputId": "b977e101-644a-4127-9f8e-41f6467d8a3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/cornellius-gp/gpytorch.git\n",
            "  Cloning https://github.com/cornellius-gp/gpytorch.git to /tmp/pip-req-build-g9vx6xvj\n",
            "  Running command git clone -q https://github.com/cornellius-gp/gpytorch.git /tmp/pip-req-build-g9vx6xvj\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from gpytorch==1.9.1.dev45+gcda4bc54) (1.13.0+cu116)\n",
            "Requirement already satisfied: linear-operator>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gpytorch==1.9.1.dev45+gcda4bc54) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from gpytorch==1.9.1.dev45+gcda4bc54) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from linear-operator>=0.2.0->gpytorch==1.9.1.dev45+gcda4bc54) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->gpytorch==1.9.1.dev45+gcda4bc54) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch==1.9.1.dev45+gcda4bc54) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch==1.9.1.dev45+gcda4bc54) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->gpytorch==1.9.1.dev45+gcda4bc54) (3.1.0)\n",
            "Building wheels for collected packages: gpytorch\n",
            "  Building wheel for gpytorch (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpytorch: filename=gpytorch-1.9.1.dev45+gcda4bc54-py3-none-any.whl size=250551 sha256=6f626fb1f24f082666686ae058c2b5001155297683efa1ca078927645403ded2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c34ticnl/wheels/b3/fe/cc/a6ff380918f3aa85afeb4f579166648926bc49eeed93812d7d\n",
            "Successfully built gpytorch\n",
            "Installing collected packages: gpytorch\n",
            "  Attempting uninstall: gpytorch\n",
            "    Found existing installation: gpytorch 1.9.0\n",
            "    Uninstalling gpytorch-1.9.0:\n",
            "      Successfully uninstalled gpytorch-1.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "botorch 0.8.0 requires gpytorch==1.9.0, but you have gpytorch 1.9.1.dev45+gcda4bc54 which is incompatible.\u001b[0m\n",
            "Successfully installed gpytorch-1.9.1.dev45+gcda4bc54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/cornellius-gp/linear_operator.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz0Ot260nbNT",
        "outputId": "cda629b3-5a9f-4c29-a4a1-f38612e136a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/cornellius-gp/linear_operator.git\n",
            "  Cloning https://github.com/cornellius-gp/linear_operator.git to /tmp/pip-req-build-1cfz2_dy\n",
            "  Running command git clone -q https://github.com/cornellius-gp/linear_operator.git /tmp/pip-req-build-1cfz2_dy\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from linear-operator==0.3.0) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->linear-operator==0.3.0) (1.21.6)\n",
            "Building wheels for collected packages: linear-operator\n",
            "  Building wheel for linear-operator (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for linear-operator: filename=linear_operator-0.3.0-py3-none-any.whl size=155628 sha256=b7a3cb3ea87b7918c76031106ecb13e5ac39d49a60209d118b240caa39dd9446\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8igs2zet/wheels/fa/3e/be/f1fdd362662b6a3d096db5b79a11e4cea55431328f3d35a371\n",
            "Successfully built linear-operator\n",
            "Installing collected packages: linear-operator\n",
            "  Attempting uninstall: linear-operator\n",
            "    Found existing installation: linear-operator 0.2.0\n",
            "    Uninstalling linear-operator-0.2.0:\n",
            "      Successfully uninstalled linear-operator-0.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "botorch 0.8.0 requires gpytorch==1.9.0, but you have gpytorch 1.9.1.dev45+gcda4bc54 which is incompatible.\n",
            "botorch 0.8.0 requires linear-operator==0.2.0, but you have linear-operator 0.3.0 which is incompatible.\u001b[0m\n",
            "Successfully installed linear-operator-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import botorch"
      ],
      "metadata": {
        "id": "r8IrvRFVng9u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from botorch.acquisition.objective import PosteriorTransform\n",
        "from botorch.models.gp_regression import MIN_INFERRED_NOISE_LEVEL\n",
        "from botorch.models.gpytorch import GPyTorchModel, MultiTaskGPyTorchModel\n",
        "from botorch.models.model import FantasizeMixin\n",
        "from botorch.models.transforms.input import InputTransform\n",
        "from botorch.models.transforms.outcome import OutcomeTransform\n",
        "from botorch.posteriors.multitask import MultitaskGPPosterior\n",
        "\n",
        "from botorch.utils.datasets import SupervisedDataset\n",
        "from gpytorch.constraints import GreaterThan,LessThan\n",
        "from gpytorch.distributions.multitask_multivariate_normal import (\n",
        "    MultitaskMultivariateNormal,\n",
        ")\n",
        "from gpytorch.distributions.multivariate_normal import MultivariateNormal\n",
        "from gpytorch.kernels.index_kernel import IndexKernel\n",
        "from gpytorch.kernels.matern_kernel import MaternKernel\n",
        "from gpytorch.kernels.multitask_kernel import MultitaskKernel\n",
        "from gpytorch.kernels.scale_kernel import ScaleKernel\n",
        "from gpytorch.likelihoods.gaussian_likelihood import (\n",
        "    FixedNoiseGaussianLikelihood,\n",
        "    GaussianLikelihood,\n",
        ")\n",
        "from gpytorch.likelihoods.multitask_gaussian_likelihood import MultitaskGaussianLikelihood\n",
        "\n",
        "from gpytorch.means import MultitaskMean\n",
        "from gpytorch.means.constant_mean import ConstantMean\n",
        "from gpytorch.models.exact_gp import ExactGP\n",
        "from gpytorch.module import Module\n",
        "from gpytorch.priors import LKJCovariancePrior\n",
        "from gpytorch.priors.wishart_prior import InverseWishartPrior\n",
        "from gpytorch.priors.prior import Prior\n",
        "from gpytorch.priors.smoothed_box_prior import SmoothedBoxPrior\n",
        "from gpytorch.priors.torch_priors import GammaPrior\n",
        "from gpytorch.settings import detach_test_caches\n",
        "from gpytorch.utils.errors import CachingError\n",
        "from gpytorch.utils.memoize import cached, pop_from_cache\n",
        "from linear_operator.operators import (\n",
        "    BatchRepeatLinearOperator,\n",
        "    CatLinearOperator,\n",
        "    DiagLinearOperator,\n",
        "    KroneckerProductDiagLinearOperator,\n",
        "    KroneckerProductLinearOperator,\n",
        "    RootLinearOperator,\n",
        "    to_linear_operator,\n",
        ")\n",
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "wzYsBGv3nic5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.optim.fit import fit_gpytorch_torch\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "\n",
        "from botorch.exceptions import BadInitialCandidatesWarning\n",
        "\n",
        "import time\n",
        "import warnings"
      ],
      "metadata": {
        "id": "oS84NqZtni8w"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "CQgIGxI8nk0V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
      ],
      "metadata": {
        "id": "sXSMLrHnnmVl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EXvjOmpnuxZ",
        "outputId": "698459dd-6910-4156-d77f-92df20ca12ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.test_functions import Hartmann, Ackley,Michalewicz,Cosine8\n",
        "from botorch.acquisition import MCAcquisitionObjective\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def outcome_constraint(samples):\n",
        "    \"\"\"L1 constraint; feasible if less than or equal to zero.\"\"\"\n",
        "    #return samples.sum(dim=-1) - 3\n",
        "    #neg_hartmann6 = Hartmann(negate=True)\n",
        "    #return neg_hartmann6(samples) - 2.9\n",
        "    ackley = Cosine8()\n",
        "\n",
        "    return ackley(samples) -0.5\n",
        "\n",
        "def weighted_obj(samples):\n",
        "\n",
        "  #neg_hartmann6 = Hartmann(negate=True)\n",
        "  ackley = Cosine8()\n",
        "  #return neg_hartmann6(samples) * (outcome_constraint(samples) <= 0).type_as(samples)\n",
        "  return ackley(samples)*(outcome_constraint(samples) <= 0).type_as(samples)\n",
        "  \n",
        "class Hartmann_Objective(MCAcquisitionObjective):\n",
        "\n",
        "    def forward(self, samples, X=None):\n",
        " \n",
        "      #neg_hartmann6 = Hartmann(negate=True, dim=6)\n",
        "      ackley = Cosine8()\n",
        "      #return neg_hartmann6(samples)\n",
        "      return ackley(samples)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "YAANCpRrnrrq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "bounds = torch.tensor([[0.0] * 6, [1.0] * 6])\n",
        "\n",
        "BATCH_SIZE = 3\n",
        "NUM_RESTARTS = 10 \n",
        "RAW_SAMPLES = 256\n",
        "\n",
        "\n",
        "def optimize_acqf_and_get_observation(acq_func):\n",
        "    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation.\"\"\"\n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=bounds,\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
        "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
        "    )\n",
        "    # observe new values \n",
        "    new_x = candidates.detach()\n",
        "    #obj = Hartmann(negate=True)\n",
        "    obj = Cosine8()\n",
        "    new_obj = obj(new_x).unsqueeze(-1)  # add output dimension\n",
        "    new_con = outcome_constraint(new_x).unsqueeze(-1)  # add output dimension\n",
        "\n",
        "    return new_x, new_obj, new_con\n",
        "\n",
        "def update_random_observations(best_random):\n",
        "    \"\"\"Simulates a random policy by taking a the current list of best values observed randomly,\n",
        "    drawing a new random point, observing its value, and updating the list.\n",
        "    \"\"\"\n",
        "    rand_x = torch.rand(BATCH_SIZE, 6)\n",
        "    next_random_best = weighted_obj(rand_x).max().item()\n",
        "    best_random.append(max(best_random[-1], next_random_best))       \n",
        "    return best_random"
      ],
      "metadata": {
        "id": "DpGlXI0Xnuv_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from botorch.acquisition.objective import ConstrainedMCObjective\n",
        "\n",
        "\n",
        "def obj_callable(Z):\n",
        "    return Z[..., 0]\n",
        "\n",
        "\n",
        "def constraint_callable(Z):\n",
        "    return Z[..., 1]\n",
        "\n",
        "\n",
        "# define a feasibility-weighted objective for optimization\n",
        "constrained_obj = ConstrainedMCObjective(\n",
        "    objective=obj_callable,\n",
        "    constraints=[constraint_callable],\n",
        ")"
      ],
      "metadata": {
        "id": "SffY8AQKnyC1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.acquisition import MCAcquisitionObjective\n",
        "import math\n",
        "from typing import Optional\n",
        "from botorch.models import KroneckerMultiTaskGP\n",
        "from botorch.acquisition.monte_carlo import MCAcquisitionFunction, MCAcquisitionObjective\n",
        "from botorch.models.model import Model\n",
        "from botorch.sampling.qmc import NormalQMCEngine \n",
        "from botorch.utils import t_batch_mode_transform\n",
        "from torch import Tensor\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "\n",
        "\n",
        "class qECI(MCAcquisitionFunction):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Model,\n",
        "        best_f: Union[float, Tensor],\n",
        "        objective: Optional[MCAcquisitionObjective] = None\n",
        "    ) -> None:\n",
        "        # we use the AcquisitionFunction constructor, since that of\n",
        "        # MCAcquisitionFunction performs some validity checks that we don't want here\n",
        "        super(MCAcquisitionFunction, self).__init__(model=model)\n",
        "\n",
        "        if objective is None:\n",
        "          self.objective = constrained_obj\n",
        "        self.best_f= best_f\n",
        "        \n",
        "        self.sampler = SobolQMCNormalSampler(sample_shape=torch.Size([256]))\n",
        "\n",
        "    @t_batch_mode_transform()\n",
        "    def forward(self, X: Tensor) -> Tensor:\n",
        "        \"\"\"Evaluate scalarized qECI on the candidate set `X`.\n",
        "\n",
        "        Args:\n",
        "            X: A `(b) x q x d`-dim Tensor of `(b)` t-batches with `q` `d`-dim\n",
        "                design points each\n",
        "        \"\"\"\n",
        "\n",
        "        #posterior = self.model.posterior(\n",
        "        #    X=X, posterior_transform=self.posterior_transform\n",
        "        #)\n",
        "        #samples = self.get_posterior_samples(posterior)\n",
        "        #obj = self.objective(samples, X=X)\n",
        "        #obj = (obj - self.best_f.unsqueeze(-1).to(obj)).clamp_min(0)\n",
        "        #q_ei = obj.max(dim=-1)[0].mean(dim=0)\n",
        "        \n",
        "       \n",
        "        #compute posterior quantities\n",
        "        posterior = self.model.posterior(X=X)\n",
        "        samples = self.get_posterior_samples(posterior)\n",
        "        \n",
        "        objective_samples = samples.matmul(torch.Tensor([1,0]).double()) \n",
        "        constraint_samples = samples.matmul(torch.Tensor([0,1]).double())  # b x q\n",
        "        \n",
        "        \n",
        "        obj = (objective_samples-self.best_f).clamp_min(0)\n",
        "        \n",
        "        obj = obj * (constraint_samples <= 0).type_as(constraint_samples)\n",
        "    \n",
        "        q_eci = obj.max(dim=-1)[0].mean(dim=0)\n",
        "\n",
        "        return q_eci\n",
        "          "
      ],
      "metadata": {
        "id": "tC2n7eayn2r1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KroneckerMultiTaskGP_m(ExactGP, GPyTorchModel, FantasizeMixin):\n",
        "    \"\"\"Multi-task GP with Kronecker structure, using an ICM kernel.\n",
        "\n",
        "    This model assumes the \"block design\" case, i.e., it requires that all tasks\n",
        "    are observed at all data points.\n",
        "\n",
        "    For posterior sampling, this model uses Matheron's rule [Doucet2010sampl] to compute\n",
        "    the posterior over all tasks as in [Maddox2021bohdo] by exploiting Kronecker\n",
        "    structure.\n",
        "\n",
        "    When a multi-fidelity model has Kronecker structure, this means there is one\n",
        "    covariance kernel over the fidelity features (call it `K_f`) and another over\n",
        "    the rest of the input parameters (call it `K_i`), and the resulting covariance\n",
        "    across inputs and fidelities is given by the Kronecker product of the two\n",
        "    covariance matrices. This is equivalent to saying the covariance between\n",
        "    two input and feature pairs is given by\n",
        "\n",
        "    K((parameter_1, fidelity_1), (parameter_2, fidelity_2))\n",
        "        = K_f(fidelity_1, fidelity_2) * K_i(parameter_1, parameter_2).\n",
        "\n",
        "    Then the covariance matrix of `n_i` parameters and `n_f` fidelities can be\n",
        "    codified as a Kronecker product of an `n_i x n_i` matrix and an\n",
        "    `n_f x n_f` matrix, which is far more parsimonious than specifying the\n",
        "    whole `(n_i * n_f) x (n_i * n_f)` covariance matrix.\n",
        "\n",
        "    Example:\n",
        "        >>> train_X = torch.rand(10, 2)\n",
        "        >>> train_Y = torch.cat([f_1(X), f_2(X)], dim=-1)\n",
        "        >>> model = KroneckerMultiTaskGP(train_X, train_Y)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_X: Tensor,\n",
        "        train_Y: Tensor,\n",
        "        likelihood: Optional[MultitaskGaussianLikelihood] = None,\n",
        "        data_covar_module: Optional[Module] = None,\n",
        "        task_covar_prior: Optional[Prior] = None,\n",
        "        rank: Optional[int] = None,\n",
        "        input_transform: Optional[InputTransform] = None,\n",
        "        outcome_transform: Optional[OutcomeTransform] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            train_X: A `batch_shape x n x d` tensor of training features.\n",
        "            train_Y: A `batch_shape x n x m` tensor of training observations.\n",
        "            likelihood: A `MultitaskGaussianLikelihood`. If omitted, uses a\n",
        "                `MultitaskGaussianLikelihood` with a `GammaPrior(1.1, 0.05)`\n",
        "                noise prior.\n",
        "            data_covar_module: The module computing the covariance (Kernel) matrix\n",
        "                in data space. If omitted, use a `MaternKernel`.\n",
        "            task_covar_prior : A Prior on the task covariance matrix. Must operate\n",
        "                on p.s.d. matrices. A common prior for this is the `LKJ` prior. If\n",
        "                omitted, uses `LKJCovariancePrior` with `eta` parameter as specified\n",
        "                in the keyword arguments (if not specified, use `eta=1.5`).\n",
        "            rank: The rank of the ICM kernel. If omitted, use a full rank kernel.\n",
        "            kwargs: Additional arguments to override default settings of priors,\n",
        "                including:\n",
        "                - eta: The eta parameter on the default LKJ task_covar_prior.\n",
        "                A value of 1.0 is uninformative, values <1.0 favor stronger\n",
        "                correlations (in magnitude), correlations vanish as eta -> inf.\n",
        "                - sd_prior: A scalar prior over nonnegative numbers, which is used\n",
        "                for the default LKJCovariancePrior task_covar_prior.\n",
        "                - likelihood_rank: The rank of the task covariance matrix to fit.\n",
        "                Defaults to 0 (which corresponds to a diagonal covariance matrix).\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            transformed_X = self.transform_inputs(\n",
        "                X=train_X, input_transform=input_transform\n",
        "            )\n",
        "        if outcome_transform is not None:\n",
        "            train_Y, _ = outcome_transform(train_Y)\n",
        "\n",
        "        self._validate_tensor_args(X=transformed_X, Y=train_Y)\n",
        "        self._num_outputs = train_Y.shape[-1]\n",
        "        batch_shape, ard_num_dims = train_X.shape[:-2], train_X.shape[-1]\n",
        "        num_tasks = train_Y.shape[-1]\n",
        "\n",
        "        if rank is None:\n",
        "            rank = num_tasks\n",
        "        if likelihood is None:\n",
        "            noise_prior = GammaPrior(1.05, 0.05)\n",
        "            noise_prior_mode = (noise_prior.concentration - 1) / noise_prior.rate\n",
        "            likelihood = MultitaskGaussianLikelihood(\n",
        "                num_tasks=num_tasks,\n",
        "                batch_shape=batch_shape,\n",
        "                noise_prior=noise_prior,\n",
        "                noise_constraint=GreaterThan(\n",
        "                    MIN_INFERRED_NOISE_LEVEL,\n",
        "                    transform=None,\n",
        "                    initial_value=noise_prior_mode,\n",
        "                ),\n",
        "                rank=kwargs.get(\"likelihood_rank\", 0),\n",
        "            )\n",
        "        if task_covar_prior is None:\n",
        "            task_covar_prior = LKJCovariancePrior(\n",
        "                n=num_tasks,\n",
        "                eta=torch.tensor(kwargs.get(\"eta\", 1.5)).to(train_X),\n",
        "                sd_prior=kwargs.get(\n",
        "                    \"sd_prior\",\n",
        "                    SmoothedBoxPrior(math.exp(-6), math.exp(1.25), 0.05),\n",
        "                ),\n",
        "            )\n",
        "        super().__init__(train_X, train_Y, likelihood)\n",
        "        self.mean_module = MultitaskMean(\n",
        "            base_means=ConstantMean(batch_shape=batch_shape), num_tasks=num_tasks\n",
        "        )\n",
        "        if data_covar_module is None:\n",
        "            data_covar_module = MaternKernel(\n",
        "                nu=2.5,\n",
        "                ard_num_dims=ard_num_dims,\n",
        "                lengthscale_prior=GammaPrior(3.0, 6.0),\n",
        "                batch_shape=batch_shape,\n",
        "            )\n",
        "        else:\n",
        "            data_covar_module = data_covar_module\n",
        "\n",
        "        self.covar_module = MultitaskKernel(\n",
        "            data_covar_module=data_covar_module,\n",
        "            num_tasks=num_tasks,\n",
        "            rank=rank,\n",
        "            batch_shape=batch_shape,\n",
        "            task_covar_prior=task_covar_prior,\n",
        "        )\n",
        "\n",
        "        if outcome_transform is not None:\n",
        "            self.outcome_transform = outcome_transform\n",
        "        if input_transform is not None:\n",
        "            self.input_transform = input_transform\n",
        "        self.to(train_X)\n",
        "\n",
        "    def forward(self, X: Tensor) -> MultitaskMultivariateNormal:\n",
        "        if self.training:\n",
        "            X = self.transform_inputs(X)\n",
        "\n",
        "        mean_x = self.mean_module(X)\n",
        "        covar_x = self.covar_module(X)\n",
        "        return MultitaskMultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _task_covar_matrix(self):\n",
        "        res = self.covar_module.task_covar_module.covar_matrix\n",
        "        if detach_test_caches.on():\n",
        "            res = res.detach()\n",
        "        return res\n",
        "\n",
        "    @property\n",
        "    @cached(name=\"train_full_covar\")\n",
        "    def train_full_covar(self):\n",
        "        train_x = self.transform_inputs(self.train_inputs[0])\n",
        "\n",
        "        # construct Kxx \\otimes Ktt\n",
        "        train_full_covar = self.covar_module(train_x).evaluate_kernel()\n",
        "        if detach_test_caches.on():\n",
        "            train_full_covar = train_full_covar.detach()\n",
        "        return train_full_covar\n",
        "\n",
        "    @property\n",
        "    @cached(name=\"predictive_mean_cache\")\n",
        "    def predictive_mean_cache(self):\n",
        "        train_x = self.transform_inputs(self.train_inputs[0])\n",
        "        train_noise = self.likelihood._shaped_noise_covar(train_x.shape)\n",
        "        if detach_test_caches.on():\n",
        "            train_noise = train_noise.detach()\n",
        "\n",
        "        train_diff = self.train_targets - self.mean_module(train_x)\n",
        "        train_solve = (self.train_full_covar + train_noise).inv_matmul(\n",
        "            train_diff.reshape(*train_diff.shape[:-2], -1)\n",
        "        )\n",
        "        if detach_test_caches.on():\n",
        "            train_solve = train_solve.detach()\n",
        "\n",
        "        return train_solve\n",
        "\n",
        "    def posterior(\n",
        "        self,\n",
        "        X: Tensor,\n",
        "        output_indices: Optional[List[int]] = None,\n",
        "        observation_noise: Union[bool, Tensor] = False,\n",
        "        posterior_transform: Optional[PosteriorTransform] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> MultitaskGPPosterior:\n",
        "        self.eval()\n",
        "\n",
        "        if posterior_transform is not None:\n",
        "            # this could be very costly, disallow for now\n",
        "            raise NotImplementedError(\n",
        "                \"Posterior transforms currently not supported for \"\n",
        "                f\"{self.__class__.__name__}\"\n",
        "            )\n",
        "\n",
        "        X = self.transform_inputs(X)\n",
        "        train_x = self.transform_inputs(self.train_inputs[0])\n",
        "\n",
        "        # construct Ktt\n",
        "        task_covar = self._task_covar_matrix\n",
        "        task_rootlt = self._task_covar_matrix.root_decomposition(\n",
        "            method=\"diagonalization\"\n",
        "        )\n",
        "        task_root = task_rootlt.root\n",
        "        if task_covar.batch_shape != X.shape[:-2]:\n",
        "            task_covar = BatchRepeatLinearOperator(\n",
        "                task_covar, batch_repeat=X.shape[:-2]\n",
        "            )\n",
        "            task_root = BatchRepeatLinearOperator(\n",
        "                to_linear_operator(task_root), batch_repeat=X.shape[:-2]\n",
        "            )\n",
        "\n",
        "        task_covar_rootlt = RootLinearOperator(task_root)\n",
        "\n",
        "        # construct RR' \\approx Kxx\n",
        "        data_data_covar = self.train_full_covar.linear_ops[0]\n",
        "        # populate the diagonalziation caches for the root and inverse root\n",
        "        # decomposition\n",
        "        data_data_evals, data_data_evecs = data_data_covar.diagonalization()\n",
        "\n",
        "        # pad the eigenvalue and eigenvectors with zeros if we are using lanczos\n",
        "        if data_data_evecs.shape[-1] < data_data_evecs.shape[-2]:\n",
        "            cols_to_add = data_data_evecs.shape[-2] - data_data_evecs.shape[-1]\n",
        "            zero_evecs = torch.zeros(\n",
        "                *data_data_evecs.shape[:-1],\n",
        "                cols_to_add,\n",
        "                dtype=data_data_evals.dtype,\n",
        "                device=data_data_evals.device,\n",
        "            )\n",
        "            zero_evals = torch.zeros(\n",
        "                *data_data_evecs.shape[:-2],\n",
        "                cols_to_add,\n",
        "                dtype=data_data_evals.dtype,\n",
        "                device=data_data_evals.device,\n",
        "            )\n",
        "            data_data_evecs = CatLinearOperator(\n",
        "                data_data_evecs,\n",
        "                to_linear_operator(zero_evecs),\n",
        "                dim=-1,\n",
        "                output_device=data_data_evals.device,\n",
        "            )\n",
        "            data_data_evals = torch.cat((data_data_evals, zero_evals), dim=-1)\n",
        "\n",
        "        # construct K_{xt, x}\n",
        "        test_data_covar = self.covar_module.data_covar_module(X, train_x)\n",
        "        # construct K_{xt, xt}\n",
        "        test_test_covar = self.covar_module.data_covar_module(X)\n",
        "\n",
        "        # now update root so that \\tilde{R}\\tilde{R}' \\approx K_{(x,xt), (x,xt)}\n",
        "        # cloning preserves the gradient history\n",
        "        updated_linear_op = data_data_covar.cat_rows(\n",
        "            cross_mat=test_data_covar.clone(),\n",
        "            new_mat=test_test_covar,\n",
        "            method=\"diagonalization\",\n",
        "        )\n",
        "        updated_root = updated_linear_op.root_decomposition().root\n",
        "        # occasionally, there's device errors so enforce this comes out right\n",
        "        updated_root = updated_root.to(data_data_covar.device)\n",
        "\n",
        "        # build a root decomposition of the joint train/test covariance matrix\n",
        "        # construct (\\tilde{R} \\otimes M)(\\tilde{R} \\otimes M)' \\approx\n",
        "        # (K_{(x,xt), (x,xt)} \\otimes Ktt)\n",
        "        joint_covar = RootLinearOperator(\n",
        "            KroneckerProductLinearOperator(\n",
        "                updated_root, task_covar_rootlt.root.detach()\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # construct K_{xt, x} \\otimes Ktt\n",
        "        test_obs_kernel = KroneckerProductLinearOperator(test_data_covar, task_covar)\n",
        "\n",
        "        # collect y - \\mu(x) and \\mu(X)\n",
        "        train_diff = self.train_targets - self.mean_module(train_x)\n",
        "        if detach_test_caches.on():\n",
        "            train_diff = train_diff.detach()\n",
        "        test_mean = self.mean_module(X)\n",
        "\n",
        "        train_noise = self.likelihood._shaped_noise_covar(train_x.shape)\n",
        "        diagonal_noise = isinstance(train_noise, DiagLinearOperator)\n",
        "        if detach_test_caches.on():\n",
        "            train_noise = train_noise.detach()\n",
        "        test_noise = (\n",
        "            self.likelihood._shaped_noise_covar(X.shape) if observation_noise else None\n",
        "        )\n",
        "\n",
        "        # predictive mean and variance for the mvn\n",
        "        # first the predictive mean\n",
        "        pred_mean = (\n",
        "            test_obs_kernel.matmul(self.predictive_mean_cache).reshape_as(test_mean)\n",
        "            + test_mean\n",
        "        )\n",
        "        # next the predictive variance, assume diagonal noise\n",
        "        test_var_term = KroneckerProductLinearOperator(\n",
        "            test_test_covar, task_covar\n",
        "        ).diag()\n",
        "\n",
        "        if diagonal_noise:\n",
        "            task_evals, task_evecs = self._task_covar_matrix.diagonalization()\n",
        "            # TODO: make this be the default KPMatmulLT diagonal method in gpytorch\n",
        "            full_data_inv_evals = (\n",
        "                KroneckerProductDiagLinearOperator(\n",
        "                    DiagLinearOperator(data_data_evals), DiagLinearOperator(task_evals)\n",
        "                )\n",
        "                + train_noise\n",
        "            ).inverse()\n",
        "            test_train_hadamard = KroneckerProductLinearOperator(\n",
        "                test_data_covar.matmul(data_data_evecs).to_dense() ** 2,\n",
        "                task_covar.matmul(task_evecs).to_dense() ** 2,\n",
        "            )\n",
        "            data_var_term = test_train_hadamard.matmul(full_data_inv_evals).sum(dim=-1)\n",
        "        else:\n",
        "            # if non-diagonal noise (but still kronecker structured), we have to pull\n",
        "            # across the noise because the inverse is not closed form\n",
        "            # should be a kronecker lt, R = \\Sigma_X^{-1/2} \\kron \\Sigma_T^{-1/2}\n",
        "            # TODO: enforce the diagonalization to return a KPLT for all shapes in\n",
        "            # gpytorch or dense linear algebra for small shapes\n",
        "            data_noise, task_noise = train_noise.linear_ops\n",
        "            data_noise_root = data_noise.root_inv_decomposition(\n",
        "                method=\"diagonalization\"\n",
        "            )\n",
        "            task_noise_root = task_noise.root_inv_decomposition(\n",
        "                method=\"diagonalization\"\n",
        "            )\n",
        "\n",
        "            # ultimately we need to compute the diagonal of\n",
        "            # (K_{x* X} \\kron K_T)(K_{XX} \\kron K_T + \\Sigma_X \\kron \\Sigma_T)^{-1}\n",
        "            #                           (K_{x* X} \\kron K_T)^T\n",
        "            # = (K_{x* X} \\Sigma_X^{-1/2} Q_R)(\\Lambda_R + I)^{-1}\n",
        "            #                       (K_{x* X} \\Sigma_X^{-1/2} Q_R)^T\n",
        "            # where R = (\\Sigma_X^{-1/2T}K_{XX}\\Sigma_X^{-1/2} \\kron\n",
        "            #                   \\Sigma_T^{-1/2T}K_{T}\\Sigma_T^{-1/2})\n",
        "            # first we construct the components of R's eigen-decomposition\n",
        "            # TODO: make this be the default KPMatmulLT diagonal method in gpytorch\n",
        "            whitened_data_covar = (\n",
        "                data_noise_root.transpose(-1, -2)\n",
        "                .matmul(data_data_covar)\n",
        "                .matmul(data_noise_root)\n",
        "            )\n",
        "            w_data_evals, w_data_evecs = whitened_data_covar.diagonalization()\n",
        "            whitened_task_covar = (\n",
        "                task_noise_root.transpose(-1, -2)\n",
        "                .matmul(self._task_covar_matrix)\n",
        "                .matmul(task_noise_root)\n",
        "            )\n",
        "            w_task_evals, w_task_evecs = whitened_task_covar.diagonalization()\n",
        "\n",
        "            # we add one to the eigenvalues as above (not just for stability)\n",
        "            full_data_inv_evals = (\n",
        "                KroneckerProductDiagLinearOperator(\n",
        "                    DiagLinearOperator(w_data_evals), DiagLinearOperator(w_task_evals)\n",
        "                )\n",
        "                .add_jitter(1.0)\n",
        "                .inverse()\n",
        "            )\n",
        "\n",
        "            test_data_comp = (\n",
        "                test_data_covar.matmul(data_noise_root).matmul(w_data_evecs).to_dense()\n",
        "                ** 2\n",
        "            )\n",
        "            task_comp = (\n",
        "                task_covar.matmul(task_noise_root).matmul(w_task_evecs).to_dense() ** 2\n",
        "            )\n",
        "\n",
        "            test_train_hadamard = KroneckerProductLinearOperator(\n",
        "                test_data_comp, task_comp\n",
        "            )\n",
        "            data_var_term = test_train_hadamard.matmul(full_data_inv_evals).sum(dim=-1)\n",
        "\n",
        "        pred_variance = test_var_term - data_var_term\n",
        "        specialized_mvn = MultitaskMultivariateNormal(\n",
        "            pred_mean, DiagLinearOperator(pred_variance)\n",
        "        )\n",
        "        if observation_noise:\n",
        "            specialized_mvn = self.likelihood(specialized_mvn)\n",
        "\n",
        "        posterior = MultitaskGPPosterior(\n",
        "            distribution=specialized_mvn,\n",
        "            joint_covariance_matrix=joint_covar,\n",
        "            test_train_covar=test_obs_kernel,\n",
        "            train_diff=train_diff,\n",
        "            test_mean=test_mean,\n",
        "            train_train_covar=self.train_full_covar,\n",
        "            train_noise=train_noise,\n",
        "            test_noise=test_noise,\n",
        "        )\n",
        "\n",
        "        if hasattr(self, \"outcome_transform\"):\n",
        "            posterior = self.outcome_transform.untransform_posterior(posterior)\n",
        "        return posterior\n",
        "\n",
        "\n",
        "    def train(self, val=True, *args, **kwargs):\n",
        "        if val:\n",
        "            fixed_cache_names = [\"data_data_roots\", \"train_full_covar\", \"task_root\"]\n",
        "            for name in fixed_cache_names:\n",
        "                try:\n",
        "                    pop_from_cache(self, name)\n",
        "                except CachingError:\n",
        "                    pass\n",
        "\n",
        "        return super().train(val, *args, **kwargs)"
      ],
      "metadata": {
        "id": "SBwjISC1iQLE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from botorch.models import SingleTaskGP, ModelListGP\n",
        "\n",
        "from gpytorch.mlls.sum_marginal_log_likelihood import ExactMarginalLogLikelihood, SumMarginalLogLikelihood\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_initial_data():\n",
        "    # generate training data\n",
        "    train_x = torch.rand(10, 6, device=device, dtype=dtype)\n",
        "\n",
        "    #obj = Hartmann(negate=True)\n",
        "    obj = Cosine8()\n",
        "    train_obj = obj(train_x).unsqueeze(-1)  # add output dimension\n",
        "    train_con = outcome_constraint(train_x).unsqueeze(-1)  # add output dimension\n",
        "\n",
        "    #exact_obj = neg_hartmann6(train_x).unsqueeze(-1)  # add output dimension\n",
        "    #exact_con = outcome_constraint(train_x).unsqueeze(-1)  # add output dimension\n",
        "    #train_obj = exact_obj + NOISE_SE * torch.randn_like(exact_obj)\n",
        "    #train_con = exact_con + NOISE_SE * torch.randn_like(exact_con)\n",
        "    \n",
        "    best_observed_value = (train_obj * (train_con <= 0).to(train_obj)).max()\n",
        "    #train_y = torch.cat((train_obj, train_con),1)\n",
        "\n",
        "    return train_x, train_obj, train_con, best_observed_value\n",
        "    \n",
        "    \n",
        "def initialize_model(train_x, train_y, state_dict=None):\n",
        "    \n",
        "    # combine into a multi-output GP model\n",
        "    model = KroneckerMultiTaskGP_m(train_x, train_y, task_covar_prior = InverseWishartPrior(2, torch.eye(2)))\n",
        "\n",
        "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
        "    # load state dict if it is passed\n",
        "    if state_dict is not None:\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model,mll\n",
        "\n",
        "\n",
        "def initialize_independent_model(train_x, train_obj, train_con, state_dict=None):\n",
        "    # define models for objective and constraint\n",
        "    model_obj = SingleTaskGP(train_x, train_obj).to(train_x)\n",
        "    model_con = SingleTaskGP(train_x, train_con).to(train_x)\n",
        "    # combine into a multi-output GP model\n",
        "    model = ModelListGP(model_obj, model_con)\n",
        "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
        "    # load state dict if it is passed\n",
        "    if state_dict is not None:\n",
        "        model.load_state_dict(state_dict)\n",
        "    return mll, model"
      ],
      "metadata": {
        "id": "_czUaK_Vn4v2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_TRIALS = 10 \n",
        "N_BATCH = 10\n",
        "\n",
        "\n",
        "verbose = False\n",
        "\n",
        "best_observed_all_ei = []\n",
        "best_observed_all_ind = []\n",
        "best_random_all = []\n",
        "\n",
        "# average over multiple trials\n",
        "for trial in range(1, N_TRIALS + 1):\n",
        "    \n",
        "    print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
        "    best_observed_ei = []\n",
        "    best_observed_ind = []\n",
        "    best_random = []\n",
        "    \n",
        "    # call helper functions to generate initial training data and initialize model\n",
        "    train_x_ei, train_obj_ei, train_con_ei, best_observed_value_ei = generate_initial_data()\n",
        "    print(best_observed_value_ei)\n",
        "    train_y_ei = torch.cat((train_obj_ei, train_con_ei),1)\n",
        "   \n",
        "    model_ei, mll_ei = initialize_model(train_x_ei, train_y_ei)\n",
        "\n",
        "    train_x_ind, train_obj_ind, train_con_ind = train_x_ei, train_obj_ei, train_con_ei\n",
        "    best_observed_value_ind = best_observed_value_ei\n",
        "    mll_ind, model_ind = initialize_independent_model(train_x_ind, train_obj_ind, train_con_ind)\n",
        "    \n",
        "\n",
        "\n",
        "    best_observed_ei.append(best_observed_value_ei)\n",
        "    best_random.append(best_observed_value_ei)\n",
        "    best_observed_ind.append(best_observed_value_ei)\n",
        "    \n",
        "    # run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "    for iteration in range(1, N_BATCH + 1):    \n",
        "        \n",
        "        t0 = time.monotonic()\n",
        "        torch.autograd.set_detect_anomaly(True)\n",
        "        fit_gpytorch_torch(mll_ei)\n",
        "        fit_gpytorch_torch(mll_ind)\n",
        "\n",
        "\n",
        "       \n",
        "        # for best_f, we use the best observed noisy values as an approximation\n",
        "        qECI_iteration = qECI(model=model_ei, best_f= (train_obj_ei * (train_con_ei <= 0).to(train_obj_ei)).max())\n",
        "        \n",
        "       \n",
        "        \n",
        "        # optimize and get new observation\n",
        "        new_x_ei, new_obj_ei, new_con_ei = optimize_acqf_and_get_observation(qECI_iteration)\n",
        "     \n",
        "                \n",
        "        # update training points\n",
        "        \n",
        "       \n",
        "        train_x_ei = torch.cat([train_x_ei, new_x_ei])\n",
        "       \n",
        "        train_obj_ei = torch.cat([train_obj_ei, new_obj_ei])\n",
        "        train_con_ei = torch.cat([train_con_ei, new_con_ei])\n",
        "\n",
        "        train_y_ei = torch.cat((train_obj_ei, train_con_ei),1)\n",
        "        # update progress\n",
        "        best_random = update_random_observations(best_random)\n",
        "        best_value_ei = (train_obj_ei * (train_con_ei <= 0).to(train_obj_ei)).max()\n",
        "        best_observed_ei.append(best_value_ei)\n",
        "\n",
        "        print(best_value_ei)\n",
        "        print(best_random)\n",
        "\n",
        "        #define the qEI and qNEI acquisition modules using a QMC sampler\n",
        "        qmc_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([256]))\n",
        "        \n",
        "        # for best_f, we use the best observed noisy values as an approximation\n",
        "        qEI = qExpectedImprovement(\n",
        "            model=model_ind, \n",
        "            best_f=(train_obj_ind * (train_con_ind <= 0).to(train_obj_ind)).max(),\n",
        "            sampler=qmc_sampler, \n",
        "            objective=constrained_obj,\n",
        "        )\n",
        "\n",
        "        # optimize and get new observation\n",
        "        new_x_ind, new_obj_ind, new_con_ind = optimize_acqf_and_get_observation(qEI)\n",
        "\n",
        "        # update training points\n",
        "        train_x_ind = torch.cat([train_x_ind, new_x_ind])\n",
        "        train_obj_ind = torch.cat([train_obj_ind, new_obj_ind])\n",
        "        train_con_ind = torch.cat([train_con_ind, new_con_ind])\n",
        "\n",
        "        best_value_ind = (train_obj_ind * (train_con_ind <= 0).to(train_obj_ind)).max()\n",
        "        best_observed_ind.append(best_value_ind)\n",
        "        print(best_value_ind)\n",
        "\n",
        "\n",
        "        # reinitialize the models so they are ready for fitting on next iteration\n",
        "        # use the current state dict to speed up fitting\n",
        "        model_ei,mll_ei = initialize_model(\n",
        "            train_x_ei, \n",
        "            train_y_ei, \n",
        "        )\n",
        "\n",
        "        mll_ind, model_ind = initialize_independent_model(\n",
        "            train_x_ind, \n",
        "            train_obj_ind, \n",
        "            train_con_ind, \n",
        "            model_ind.state_dict(),\n",
        "        )\n",
        "        \n",
        "       \n",
        "        t1 = time.monotonic()\n",
        "        \n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"\\nBatch {iteration:>2}: best_value (random, qEI, qNEI) = \"\n",
        "                f\"({best_value_ei:>4.2f}), \"\n",
        "                f\"time = {t1-t0:>4.2f}.\", end=\"\"\n",
        "            )\n",
        "        else:\n",
        "            print(\".\", end=\"\")\n",
        "   \n",
        "    best_observed_all_ei.append(best_observed_ei)\n",
        "    best_random_all.append(best_random)\n",
        "    best_observed_all_ind.append(best_observed_ind)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp88MV4Nn8Mt",
        "outputId": "24ab3dfc-b6bb-4c19-aab3-628b23783cba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial  1 of 10 tensor(-0.7366, dtype=torch.float64)\n",
            "tensor(-0.6400, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(-0.6069, dtype=torch.float64)\n",
            ".tensor(-0.1839, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(-0.2258, dtype=torch.float64)\n",
            ".tensor(-0.0849, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(0.2057, dtype=torch.float64)\n",
            ".tensor(-0.0402, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(0.2954, dtype=torch.float64)\n",
            ".tensor(0.2162, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(0.3745, dtype=torch.float64)\n",
            ".tensor(0.2163, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(0.4329, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2163, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(0.4329, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2163, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(0.4329, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2163, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(0.4902, dtype=torch.float64)\n",
            ".tensor(0.2163, dtype=torch.float64)\n",
            "[tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64), tensor(-0.7366, dtype=torch.float64)]\n",
            "tensor(0.4902, dtype=torch.float64)\n",
            ".\n",
            "Trial  2 of 10 tensor(-1.0298, dtype=torch.float64)\n",
            "tensor(-1.0298, dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64)]\n",
            "tensor(-0.5331, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0., dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64)]\n",
            "tensor(-0.5331, dtype=torch.float64)\n",
            ".tensor(0., dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), -0.8720291256904602]\n",
            "tensor(-0.4240, dtype=torch.float64)\n",
            ".tensor(0.4776, dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), -0.8720291256904602, -0.8720291256904602]\n",
            "tensor(-0.2151, dtype=torch.float64)\n",
            ".tensor(0.4776, dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), -0.8720291256904602, -0.8720291256904602, -0.8194212317466736]\n",
            "tensor(0.1184, dtype=torch.float64)\n",
            ".tensor(0.4776, dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), -0.8720291256904602, -0.8720291256904602, -0.8194212317466736, -0.8194212317466736]\n",
            "tensor(0.2376, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4776, dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), -0.8720291256904602, -0.8720291256904602, -0.8194212317466736, -0.8194212317466736, -0.8194212317466736]\n",
            "tensor(0.2376, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4776, dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), -0.8720291256904602, -0.8720291256904602, -0.8194212317466736, -0.8194212317466736, -0.8194212317466736, -0.7403824329376221]\n",
            "tensor(0.4054, dtype=torch.float64)\n",
            ".tensor(0.4776, dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), -0.8720291256904602, -0.8720291256904602, -0.8194212317466736, -0.8194212317466736, -0.8194212317466736, -0.7403824329376221, -0.7403824329376221]\n",
            "tensor(0.4070, dtype=torch.float64)\n",
            ".tensor(0.4776, dtype=torch.float64)\n",
            "[tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), tensor(-1.0298, dtype=torch.float64), -0.8720291256904602, -0.8720291256904602, -0.8194212317466736, -0.8194212317466736, -0.8194212317466736, -0.7403824329376221, -0.7403824329376221, -0.7403824329376221]\n",
            "tensor(0.4070, dtype=torch.float64)\n",
            ".\n",
            "Trial  3 of 10 tensor(-0.2700, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.2002, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64)]\n",
            "tensor(0.1853, dtype=torch.float64)\n",
            ".tensor(0.1487, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64)]\n",
            "tensor(0.1853, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4515, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64)]\n",
            "tensor(0.2257, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4515, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64)]\n",
            "tensor(0.2257, dtype=torch.float64)\n",
            ".tensor(0.4515, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64)]\n",
            "tensor(0.3971, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4515, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64)]\n",
            "tensor(0.4974, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4515, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64)]\n",
            "tensor(0.4974, dtype=torch.float64)\n",
            ".tensor(0.4515, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), -0.16936138272285461]\n",
            "tensor(0.4974, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4898, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), -0.16936138272285461, -0.16936138272285461]\n",
            "tensor(0.4974, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4898, dtype=torch.float64)\n",
            "[tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), tensor(-0.2700, dtype=torch.float64), -0.16936138272285461, -0.16936138272285461, -0.16936138272285461]\n",
            "tensor(0.4974, dtype=torch.float64)\n",
            ".\n",
            "Trial  4 of 10 tensor(-0.5482, dtype=torch.float64)\n",
            "tensor(-0.5482, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64)]\n",
            "tensor(-0.5482, dtype=torch.float64)\n",
            ".tensor(-0.3176, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493]\n",
            "tensor(-0.2138, dtype=torch.float64)\n",
            ".tensor(0.0373, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493, -0.4175974130630493]\n",
            "tensor(-0.0657, dtype=torch.float64)\n",
            ".tensor(0.4330, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493, -0.4175974130630493, -0.4175974130630493]\n",
            "tensor(0.0860, dtype=torch.float64)\n",
            ".tensor(0.4330, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493]\n",
            "tensor(0.3002, dtype=torch.float64)\n",
            ".tensor(0.4330, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493]\n",
            "tensor(0.4522, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4330, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493]\n",
            "tensor(0.4522, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4330, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493]\n",
            "tensor(0.4522, dtype=torch.float64)\n",
            ".tensor(0.4330, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493]\n",
            "tensor(0.4522, dtype=torch.float64)\n",
            ".tensor(0.4330, dtype=torch.float64)\n",
            "[tensor(-0.5482, dtype=torch.float64), tensor(-0.5482, dtype=torch.float64), -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493, -0.4175974130630493]\n",
            "tensor(0.4522, dtype=torch.float64)\n",
            ".\n",
            "Trial  5 of 10 tensor(-1.4346, dtype=torch.float64)\n",
            "tensor(-1.1021, dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64)]\n",
            "tensor(-1.1003, dtype=torch.float64)\n",
            ".tensor(-0.9644, dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64)]\n",
            "tensor(-0.6714, dtype=torch.float64)\n",
            ".tensor(-0.9644, dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), -1.2730791568756104]\n",
            "tensor(-0.6714, dtype=torch.float64)\n",
            ".tensor(0., dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), -1.2730791568756104, -1.2730791568756104]\n",
            "tensor(-0.5779, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0., dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), -1.2730791568756104, -1.2730791568756104, -0.5045345425605774]\n",
            "tensor(-0.4077, dtype=torch.float64)\n",
            ".tensor(0., dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), -1.2730791568756104, -1.2730791568756104, -0.5045345425605774, -0.5045345425605774]\n",
            "tensor(-0.1388, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3631, dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), -1.2730791568756104, -1.2730791568756104, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774]\n",
            "tensor(-0.1182, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4404, dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), -1.2730791568756104, -1.2730791568756104, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774]\n",
            "tensor(-0.0869, dtype=torch.float64)\n",
            ".tensor(0.4404, dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), -1.2730791568756104, -1.2730791568756104, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774]\n",
            "tensor(-0.0461, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4404, dtype=torch.float64)\n",
            "[tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), tensor(-1.4346, dtype=torch.float64), -1.2730791568756104, -1.2730791568756104, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774, -0.5045345425605774]\n",
            "tensor(-0.0461, dtype=torch.float64)\n",
            ".\n",
            "Trial  6 of 10 tensor(-0.3481, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.3481, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(-0.2646, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.3481, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(-0.2646, dtype=torch.float64)\n",
            ".tensor(0.0532, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(-0.0814, dtype=torch.float64)\n",
            ".tensor(0.2779, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(-0.0574, dtype=torch.float64)\n",
            ".tensor(0.2779, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(0.1019, dtype=torch.float64)\n",
            ".tensor(0.2779, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(0.2282, dtype=torch.float64)\n",
            ".tensor(0.4510, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(0.3088, dtype=torch.float64)\n",
            ".tensor(0.4510, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(0.3694, dtype=torch.float64)\n",
            ".tensor(0.4510, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(0.4208, dtype=torch.float64)\n",
            ".tensor(0.4510, dtype=torch.float64)\n",
            "[tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64), tensor(-0.3481, dtype=torch.float64)]\n",
            "tensor(0.4208, dtype=torch.float64)\n",
            ".\n",
            "Trial  7 of 10 tensor(-0.5469, dtype=torch.float64)\n",
            "tensor(-0.4378, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985]\n",
            "tensor(-0.5469, dtype=torch.float64)\n",
            ".tensor(-0.3347, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(-0.5469, dtype=torch.float64)\n",
            ".tensor(-0.3347, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(-0.5006, dtype=torch.float64)\n",
            ".tensor(-0.3023, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(-0.3942, dtype=torch.float64)\n",
            ".tensor(-0.1468, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(-0.3942, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2315, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(-0.0971, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4472, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(0.0679, dtype=torch.float64)\n",
            ".tensor(0.4472, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(0.0679, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4472, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(0.2595, dtype=torch.float64)\n",
            ".tensor(0.4472, dtype=torch.float64)\n",
            "[tensor(-0.5469, dtype=torch.float64), -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985, -0.2443864941596985]\n",
            "tensor(0.2595, dtype=torch.float64)\n",
            ".\n",
            "Trial  8 of 10 tensor(-1.0579, dtype=torch.float64)\n",
            "tensor(-0.6356, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64)]\n",
            "tensor(-0.9478, dtype=torch.float64)\n",
            ".tensor(-0.5280, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64)]\n",
            "tensor(-0.5535, dtype=torch.float64)\n",
            ".tensor(-0.1398, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), -0.8839031457901001]\n",
            "tensor(-0.2662, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.1398, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), -0.8839031457901001, -0.8839031457901001]\n",
            "tensor(-0.2662, dtype=torch.float64)\n",
            ".tensor(-0.1077, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), -0.8839031457901001, -0.8839031457901001, -0.8839031457901001]\n",
            "tensor(-0.0198, dtype=torch.float64)\n",
            ".tensor(0.1142, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001]\n",
            "tensor(-0.0198, dtype=torch.float64)\n",
            ".tensor(0.4174, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001]\n",
            "tensor(-0.0198, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4913, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001]\n",
            "tensor(-0.0198, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4913, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001]\n",
            "tensor(-0.0198, dtype=torch.float64)\n",
            ".tensor(0.4913, dtype=torch.float64)\n",
            "[tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), tensor(-1.0579, dtype=torch.float64), -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001, -0.8839031457901001]\n",
            "tensor(0.0603, dtype=torch.float64)\n",
            ".\n",
            "Trial  9 of 10 tensor(-1.1194, dtype=torch.float64)\n",
            "tensor(-0.7840, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64)]\n",
            "tensor(-0.5267, dtype=torch.float64)\n",
            ".tensor(-0.4012, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64)]\n",
            "tensor(-0.4003, dtype=torch.float64)\n",
            ".tensor(-0.3418, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64)]\n",
            "tensor(-0.0036, dtype=torch.float64)\n",
            ".tensor(-0.3371, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64)]\n",
            "tensor(0.2319, dtype=torch.float64)\n",
            ".tensor(-0.2310, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64)]\n",
            "tensor(0.3981, dtype=torch.float64)\n",
            ".tensor(-0.0241, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64)]\n",
            "tensor(0.4484, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.0010, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64)]\n",
            "tensor(0.4484, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2251, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64)]\n",
            "tensor(0.4484, dtype=torch.float64)\n",
            ".tensor(0.4254, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), -0.9280710816383362]\n",
            "tensor(0.4484, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4500, dtype=torch.float64)\n",
            "[tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), tensor(-1.1194, dtype=torch.float64), -0.9280710816383362, -0.9280710816383362]\n",
            "tensor(0.4484, dtype=torch.float64)\n",
            ".\n",
            "Trial 10 of 10 tensor(-0.6662, dtype=torch.float64)\n",
            "tensor(-0.6029, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64)]\n",
            "tensor(-0.4830, dtype=torch.float64)\n",
            ".tensor(-0.3611, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193]\n",
            "tensor(-0.2600, dtype=torch.float64)\n",
            ".tensor(-0.3082, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193, -0.2650241255760193]\n",
            "tensor(0.0241, dtype=torch.float64)\n",
            ".tensor(-0.2554, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193, -0.2650241255760193, -0.2650241255760193]\n",
            "tensor(0.0592, dtype=torch.float64)\n",
            ".tensor(-0.2110, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193]\n",
            "tensor(0.1241, dtype=torch.float64)\n",
            ".tensor(-0.2110, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193]\n",
            "tensor(0.4401, dtype=torch.float64)\n",
            ".tensor(-0.2110, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193]\n",
            "tensor(0.4401, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3108, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193]\n",
            "tensor(0.4401, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4109, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193]\n",
            "tensor(0.4403, dtype=torch.float64)\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:306: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
            "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2.')]\n",
            "Trying again with a new set of initial conditions.\n",
            "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/botorch/optim/optimize.py:328: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4109, dtype=torch.float64)\n",
            "[tensor(-0.6662, dtype=torch.float64), tensor(-0.6662, dtype=torch.float64), -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193, -0.2650241255760193]\n",
            "tensor(0.4403, dtype=torch.float64)\n",
            "."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def ci(y):\n",
        "    return 1.96 * y.std(axis=0) / np.sqrt(N_TRIALS)\n",
        "\n",
        "\n",
        "obj = Cosine8()\n",
        "#obj = Hartmann(negate=True, dim=6)\n",
        "GLOBAL_MAXIMUM = obj.optimal_value\n",
        "\n",
        "print(GLOBAL_MAXIMUM)\n",
        "\n",
        "iters = np.arange(N_BATCH + 1) * BATCH_SIZE\n",
        "y_ei = np.asarray(best_observed_all_ei)\n",
        "y_rnd = np.asarray(best_random_all)\n",
        "y_ind = np.asarray(best_observed_all_ind)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "\n",
        "ax.errorbar(iters, y_rnd.mean(axis=0), yerr=ci(y_rnd), label=\"random\", linewidth=1.5)\n",
        "ax.errorbar(iters, y_ei.mean(axis=0), yerr=ci(y_ei), label=\"qECI\", linewidth=1.5)\n",
        "ax.errorbar(iters, y_ind.mean(axis=0), yerr=ci(y_ind), label=\"qEI\", linewidth=1.5)\n",
        "\n",
        "plt.plot([0, N_BATCH * BATCH_SIZE], [GLOBAL_MAXIMUM] * 2, 'k', label=\"true best objective\", linewidth=2)\n",
        "#ax.set_ylim(bottom=0.5)\n",
        "ax.set(xlabel='number of observations (beyond initial points)', ylabel='best objective value')\n",
        "ax.legend(loc=\"lower right\")"
      ],
      "metadata": {
        "id": "g4Y9aXUioxgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "857cba30-a8df-4d3d-9ff2-182a2523f06c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb779321b20>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAFzCAYAAABIJrEIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xUVfrH8c9JJY0QWgiQUBSkF42AYkMQsbK6ir2g/rCsa2HFXlFXsC+rgliwrYoFhBUVEURWaVKUrnQSSkJJQnqZOb8/7hACJGGATCbJfN+vV14ztz+z6j7PPfecc421FhEREQksQf4OQERERKqfCgAREZEApAJAREQkAKkAEBERCUAqAERERAKQCgAREZEAFOLvAKpT48aNbevWrf0dhoiISLVYvHjxLmttk/K2BVQB0Lp1axYtWuTvMERERKqFMWZzRdv0CEBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREAlBAvQugKhlj/B2CiIjUQdbaarmOWgBEREQCkFoAjlJ1VWgiIiK+oBYAERGRAKQCQEREJACpABAREQlAfi0AjDGDjDF/GGPWGWMeLGf7K8aY3zx/fxpjMstsc5XZNrV6IxcREand/NYJ0BgTDLwOnAOkAr8aY6Zaa1ft28dae2+Z/f8O9CxzinxrbY/qildERKQu8WcLQC9gnbV2g7W2CPgUGFzJ/lcBn1RLZCIiInWcPwuAFkBKmeVUz7pDGGNaAW2AWWVW1zPGLDLGzDfG/MV3YYqIiNQ9tWUegCuBL6y1rjLrWllrtxpj2gKzjDHLrbXrDz7QGDMMGAaQlJRUPdGKiIjUcP5sAdgKJJZZbulZV54rOaj531q71fO5AZjNgf0Dyu433lqbbK1NbtKkybHGLCIiUif4swD4FWhnjGljjAnDSfKH9OY3xnQA4oB5ZdbFGWPCPd8bA32BVQcfKyIiIuXz2yMAa22JMeZOYDoQDLxrrV1pjBkJLLLW7isGrgQ+tQfOvdsReNMY48YpYkaVHT0gIiIilTOBNKd9cnKyXbRokb/DEBERqRbGmMXW2uTytmkmQBERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQCkAkBERCQAqQAQEREJQCoAREREApAKABERkQDk1wLAGDPIGPOHMWadMebBcrbfaIzZaYz5zfN3S5ltNxhj1nr+bqjeyEVERGq3EH9d2BgTDLwOnAOkAr8aY6Zaa1cdtOtEa+2dBx3bEHgCSAYssNhzbEY1hC4iIlLr+bMFoBewzlq7wVpbBHwKDPby2HOBGdbaPZ6kPwMY5KM4RURE6hx/FgAtgJQyy6medQf7qzFmmTHmC2NM4hEeizFmmDFmkTFm0c6dO6sibhERkVqvpncC/C/Q2lrbDecu//0jPYG1dry1Ntlam9ykSZMqD1BERKQ28mcBsBVILLPc0rOulLV2t7W20LP4NnCSt8eKiIhIxfxZAPwKtDPGtDHGhAFXAlPL7mCMSSizeDGw2vN9OjDQGBNnjIkDBnrWiYiIiBf8NgrAWltijLkTJ3EHA+9aa1caY0YCi6y1U4G7jDEXAyXAHuBGz7F7jDFP4xQRACOttXuq/UeIiEjtMOEC53PoNP/GUZlqjtFvBQCAtfYb4JuD1j1e5vtDwEMVHPsu8K5PAxQREamjanonQBEREfEBv7YAiIhIHVHDm9iHmjQAJvg5jspUd4xqARAREQlAKgBEREQCkB4BiIjUdDW8eb1OshasG9wu59O6Dlp2l1n2dps9aPnAbd3yctkbHFxtP1EFgIiIHLMa84zdWshJh4yNsGcj7NkAGRt5dFsKca4SeLH9ocm6vCSPrfbQ7wUWR0ZV2/VUAIiISO3iKoG9qU6Cz/Ak+T0bIWOT81mcu39fEwSxLSkIMqwMi+D0E84DE+ysD/J87vsrXS5ne7nbzEHLlW0re2zQ/uUy20Z+M5TcoKDSKW99TQWAiIjUPMUFTkLfdydfNtFnbgF38f59g8MhrjU0bAOtT4eGbZ3vcW2gQRKEhPHie8kAnH7Rv/zyc7yxMbxetV5PBYCIiPhHQdYBzfTOd0+y37uNA5rhw+s7Sb1ZV+h0sZPk49o462KaO3fVckRUAIiIiG9U8Dy+9Hv+QTO4RzV1EnubMzzJvcydfGRDp1ldqowKABGRGq7GdLCriNvN8QX5tCwugu8fPezzeBq2hU6D9yf3hm2dJvzwaH/9goCkAkBEAt7Q74YCMGFQjU2xNdOutfD7p7BsIo9kpTrrFozf/zy+9E7ek+RjEyEkzK8hy34qAERExHu5u2HlJPj9E9i62Lmrb9uPN0OL+LNeBC/dtFTP42sJFQAiIlK5kkL4c7pzt7/2e6cHfnwXGPgMdL0cYpox39PLXsn/GDTrWq2XUwEgIiKHshZSf3Xu9FdMgoJMiI6H3rdC9yurPVkds9oWbzVQASAiIvvt2QjLPoNlnzo99UMioOOFTtJvcxYEK23UFfonKSIS6PIzYdVX8PtE2DLXWdf6dDj9H9DxYqhX37/xiU+oABARCUSuYlg307nTX/MNuAqhUTs4+zHoNsSZQU/qNBUAIiKBwlrY/rvTmW/555C3CyIawkk3OE38zU/UZDsBRAWAiEhdl7UVln/mJP6dayA4DNoPgu5XwfEDNDbfS27rptBVSJGriEJXIYUlhc6ne//30m3l/BW5iigoKThknyJXEQWuAtbsXkN0WPVNhqQCQESkLirMgdX/dXrxb5wDWEjsDRe+Ap0vgYg4f0foc9ZadhfsJjU7lV35uyhyFTFmyZhDEm+5CbmcRF1c9gVERyEkKITw4PAD/sKCw6gXXI+w4DCCg4IJNsFV9Ou9iKfariQiIr7ldsHGn5w7/dX/heI8aNAKznzAea7f6Dh/R1jl3NZNel46KdkpbNm7hS3ZW0jJTildzivJO2D/d1a8U27y3bcuKiSKhuENCQ+peJ/S9SGe9UHhpftXlNzDgsMICao85e6bkbK6qAAQEdmx3N8RHJu0Vc6d/vLPIXs7hMc6Cb/blZDUp3qe6/twnH2Ju4TtudtJ2ZvCluwySX6vk+iL3EWl+4YEhdAyuiWJMYmc2PREkuonkRiTyNjfxhIWHMZ7g97DqJ8DoAJARKR2ykl3Ev7vn8KOZRAUAsefA4Oeg/bnQWj1vlv+WBW5ikjNSd2f5PduISXHSfLbcrZRYktK960XXI+WMS1pVb8Vp7U4rTTJJ9VPollkM4KDDm1Gn7DCec+Dkv9+KgBERGqL4nz44xsn6a+bCdYFzXvCoNHQ5a8Q3cTfEVYqrzhvf/O8J8mnZqeyJXsLO3J3YLGl+0aHRpMYk0iHhh0Y2HogSTFOkk+MSaRJZBOCjKYcPlYqAEREarg2hQWclZ0FL7aHwr1QvwX0vctp4m/awd/hHSCrMKs0qe97Jr9veVf+rgP2jQuPI7F+IifGn1ia4PfdzceFx+lu3cdUAIiI1ESuElg9FeaP5fHtKeQbA92ucsbrtz4Nymnm9rWcohzS89JJy0sjPS/9gO+rdq+i0FXIaZ+edsAxTSOaklg/0Wmqj0kisX5i6Z18/TDNMOhPKgBERGqS/AxY/D4sfAv2pkJcGz5q2IRfousz9pKxPrmky+1id8HuQ5J7el46ablppesO7lEPEBMWQ3xkPCFBIUSFRnF9p+tLk3zL6JZEhkb6JGY5dioARERqgl3rYMFY+O1jZ/he69Ph/Beg/bnM/KD3UZ82rziv3Dv2ssu783fjsq4DjgsxITSObEzTyKa0i2tH3xZ9aRrZlKaRTYmPjC/9HhESAewfwnZjlxuPOlapXioARET8xVrYMBvmj4W1050Z+rpeDr1vg4RulR7qtm72FOxxEnpu+ck9PS+d7OLsQ46NDo0uTeB9EvqUJvT4yHiaRjmfceFx5faml7pDBYCI+NaEC5zPodP8G0dNUpzvDOGbPxbSV0FkYzjzQUi+CWLiKXIVkZadUtoEv4MSioDhs4eXJvldebsOGBoHEGSCaBzRmPjIeFrHtqZXQq9D7tjjI+PVLC+ACgAR8bGhJg2ACX6Oo0bI3kHegnHs+P1D0oqzSW/YirQ+15IW04S0gq2kz/47aXlp7CnYc+BxBoIsrM1YS3xkPCfHn+wk86j4AxJ8o3qNdNcuXvNrAWCMGQT8CwgG3rbWjjpo+3DgFqAE2AncZK3d7NnmAvZN37XFWntxtQUuInIQay17i/ayI3cHaXn7O86l5aaRlrGOtIx1pJfkkh0UBI0jgUigANLm0CCzAfGR8cRHxdO5cWfnu2c5PjKep74aQjCGCZf8198/U+oQvxUAxphg4HXgHCAV+NUYM9Vau6rMbkuBZGttnjHmduB54ArPtnxrbY9qDVpEApLL7WJPwR7S89LZkbfjgJ7xaXlppcuFrsIDjjMYmtggmhbl0doNvRu2J77VmTRt0pH4yHiaRTajSWQT6oVUPmtfMBoPL1XPny0AvYB11toNAMaYT4HBQGkBYK39scz+84FrqzVCEQkIxVgKsHy36bvSZJ6Wuz/B78zbecjz9pCgkNI79U6NOtEvsZ9zxx5an6Ypi2m2fDKNMjYTGpsEvf8OPa+DiAZ++oUih/JnAdACSCmznApUNtblZuDbMsv1jDGLcB4PjLLWflXeQcaYYcAwgKSkpGMKWETqlqzCLMYsGcPvFIGBET+NACAiJKI0uZ/c7OQDesnva5aPqxd34HS0ezbCwvGw5EMoyobEPjDgaehwIQSru5XUPLXi30pjzLVAMnBmmdWtrLVbjTFtgVnGmOXW2vUHH2utHQ+MB0hOTrYHbxep1dTD/qi4rZsp66bwyuJXyCrKoinBxNogRg+eSHxUPDGhMd5NQ2stbJ4L89+ANdOc2fk6XwJ9bocWJ/n+h4gcA38WAFuBxDLLLT3rDmCMGQA8ApxprS19wGat3er53GCMmQ30BA4pAEREyvpjzx88u+BZlqYvpXuT7jza51FGT3WeLraLa+fdSUqKYOUkmPe68ya+iDg47V7o9X9Qv7kPoxepOv4sAH4F2hlj2uAk/iuBq8vuYIzpCbwJDLLWppdZHwfkWWsLjTGNgb44HQRFRMqVU5TD67+9zidrPqF+WH1GnjqSwccPPrK3yuXugkXvwq9vQ04aND4BLnwVul0BYRpbL8dmwqDqHSzrtwLAWltijLkTmI4zDPBda+1KY8xIYJG1dirwAhANfO5pjts33K8j8KYxxg0E4fQBWFXuhUQkoFlr+Xbjt7y46EV25e/isvaXcfeJdxMbHuv9SdJWOpP2LPsMXIVw/ADo8wYc1x/0xjqppfzaB8Ba+w3wzUHrHi/zfUAFx80Fuvo2OhGp7TZkbuCfC/7Jgh0L6NSoE2POHkOXxl28O9jthnUznOf7G2ZDSAT0uNqZpreGvYJX5GjUik6AIiJHIq84jzeXvckHqz4gIiSCR3s/ymXtL/NulrzCHPj9E1gwDnavg5gE6P84nDQUIhv6PnjxiepuXq8NVACISJ1hrWXmlpmM/nU0O3J3MPi4wdx70r00imh02GMblhTD94/BkvehIAuanwh/fQc6DYbg0GqIXqR6qQAQkTphy94tPLfwOX7e+jPt4tox+vTRnBh/YuUHFefDupncnr6dk/JyYOtr0PEi6PM3SOyl5/tSp3ldABhjIq21eb4MRkTkSBWUFPDuind5Z/k7hAaHMiJ5BFd3vJqQoAr+760oF9bOgFVT4M/pUJxLp6Agvq/fgPNu+h800IRhEhgOWwAYY04F3sbpjZ9kjOkO3GqtvcPXwYmIVGZO6hyeW/AcqTmpnNf6PO47+T6aRjY9dMfCHFg73Un6a2dAcZ7zCt5uQ6DTYO6Zcx8uYzhPyV8CiDctAK8A5wJTAay1vxtjzvBpVCIildiWs43RC0czK2UWbWLb8NbAt+iT0OfAnQqynDv8VVNg3Q9QUgDR8U5P/k5/gaRTSqfodf1PTf0SeLx6BGCtTTloWkyXb8IREalYsauY91e9z5u/v4kxhrtPvJsbOt1A6L5OevkZ8Me3TtJfPwtcRRDTHE660enMl9jbma5XRLwqAFI8jwGsMSYUuBtY7duwREQONH/7fJ6d/yyb9m6if1J/Hjj5ARKiEyBvjzMP/6qvnPH67hKITYRew5yk3yIZgo5gtj85KhpmV/t4UwDcBvwL5+19W4Hvgb/5MigRkX3S89J58dcX+XbTtyTGJPJG/zc4vUEHWPO1c6e/cQ5YFzRoBX3ugM5/cYbw1aUe/M0075lUvcMWANbaXcA11RCLiEipEncJH6/+mDd+f4NiVzF3dLyem9zRhP8wGjb/AtYNDdtC37udO/2E7nUr6Yv4mDejACYAh7xG11p7k08iEpGAtyRtCc8seIa1GWs5LaoVD2cXkfjNs4CFxu3h9PucpB/fWUlf5Ch58wjg6zLf6wGXANt8E46IBLLd+bt5ed4zTE35gWY2iFfTd3J23hZM005w1oNO0m/a0d9hitQJ3jwC+LLssjHmE+Bnn0UkIgHHtXsdny94gTHp88jHzc1ZexkWnkhkn2uh42Bo0t7fIfqVOtiJLxzNVMDtgHJm2hAROQK718Oqr1i++kueYRerwsPp7Q7h4VZ/oe1FN0Cj4/wdoUid5k0fgGycPgDG87kDeMDHcYlIXbTzT6fn/qopZO1cyatxsXwZE0PjkDie7/E3BnW+DqNn+iLVwptHADHVEYiI1EGuEloVFtAjLxde7wM7V+MGpiR145W2x7PXXcS1Ha/hju53EB0W7e9oRQJKhQWAMabS12hZa5dUfTgiUquVFMK2pc4wvc1zYcsCnizKxg3QqhNr+t3PM3t/5/c9q+nZqCeP9H6EExqe4O+oRQJSZS0AL1WyzQJnV3EsInKEhpo0APzWRawoF1J/dZL95rnO95ICZ1uTDtDtcsZt+obl9eqR2Pl0PlnzCQ3CG/B036e5+LiLCTKaoU/EXyosAKy1/aozEBGpBfIzYct82OJJ+NuWOlPvmiBo1g2Sb4ZWpzov2olqhLWWb9+fTQol/G/1xww5YQh/7/l3YsNj/f1LRAKeV6MAjDFdgE448wAAYK39wFdBiUgNkZO+/+5+81xIWwFYCAqFFifBqXdBq76Q2Avq1T/g0LTcNJ6Z/wwbTAmR1vDhBZ/QuXFn//yOw5hg4/0dgki182YUwBPAWTgFwDfAeTjzAKgAEKlrMlM8yd7zDH/3Wmd9SIST5M96yLnDb5kMoRHlnsJay6S1k3hp0UsUu4tpaYOJJ7jGJn+RQOVNC8BlQHdgqbV2qDEmHvjIt2GJiM9Z64zF35fsN8+FrC3OtvBYSOoDPa917vATukNI2GFPmZqdylPznmL+9vkkxyfz1KlP8cSkS338Q0TkaHhTAORba93GmBJjTH0gHUj0cVwiUtXcbkhfCZvn7U/6uenOtqgmzp39qXc6n007QVCw96e2bj5Z8wn/WvIvDIbH+jzGZe0vUyc/kRrMmwJgkTGmAfAWsBjIAeb5NCoROXauYtj+e5khefOgIMvZVr8lHNfPSfat+kKj44/6pTobszbyxNwnWJq+lL4t+vJEnydIiE6owh8iIr7gzURAd3i+jjPGfAfUt9Yu821YInLEivNh6+L9z/BTFkJxnrOt0fHOi3Ra9XWSfoOkY75cibuE91a+x9jfxlIvpB7PnvYsF7W9SDP5idQS3nQCnAp8Ckyx1m7yeUQi4rVIl4tz92bCu4Oc5O8qAgzEd4Ge1+0fkhdTtb3c/9jzB4/98hir96zmnFbn8HDvh2kc0bhKr1Gthk7zdwQi1c6bRwAvAVcAzxljfsUpBr621hb4NDIRqVxmCg/vSKVZcRFEF0Pv25w7/KTeEBHnk0sWuYoYv2w87yx/h/rh9XnpzJcY2HqgT64lIr7lzSOAn4CfjDHBOLP//R/wLlC/0gNF6oIJFzifNe0OMW0lfHQZcSUlvBDfggf/b6bPL7ls5zIe/+Vx1met56K2F3H/yffToF4Dn19XRHzD24mAIoCLcFoCTgTe92VQIlKJTT/DJ1dDWCTPJbQkNSzcp5fLL8nntaWv8dHqj2gS0YTX+7/OGS3P8Ok1RcT3vOkD8BnQC/gOeA34yVrr9nVgIlKOlZNh0jCIawPXfknqV5f49HK/7viVJ+Y+QUp2Cpe3v5zhJw3XW/tE6ghvWgDeAa6y1rp8HYyIVGL+OPjuQUjsDVd9ApENfXapnKIcXl3yKhP/mEjL6Ja8M/AdeiX08tn1RKT6edMHYHp1BCIiFXC7YeaT8Mu/oMOF8Ne3K5yGtyr8vPVnnpr3FGm5aVzX6Tru7HEnkaGRPrueiPiHX6fpMsYMMsb8YYxZZ4x5sJzt4caYiZ7tC4wxrctse8iz/g9jzLnVGbcEjqEmrfSVu35RUgRf3eYk/+SbYcgHPkv+WYVZPPLzI9z+w+1EhkTy4fkfcv/J9yv5i9RRXnUC9AXPqILXgXOAVOBXY8xUa+2qMrvdDGRYa483xlwJjAauMMZ0Aq4EOgPNgR+MMe31mELqlMJs+Ox6WD8Lzn4UTr/vqGfrO5wZm2fw7PxnySrMYli3Ydza7VbCgg8/97+I1F7edAI0wDVAW2vtSGNMEtDMWrvwGK/dC1hnrd3guc6nwGCgbAEwGHjS8/0L4DVPPIOBT621hcBGY8w6z/k0RbHUDdlp8PHlsGMFDH7deSmPD+zK38U/F/yTGZtn0LFhR8adM44ODTv45FoiUrN40wLwBuDGmQNgJJANfAmcfIzXbgGklFlOBXpXtI+1tsQYkwU08qyff9CxLY4xHpGaYfd6+PASyN0JV0+EdudU+SWstXy94WtG/zqavOI87j7xbm7ofAOhQaFVfi0RqZm8KQB6W2tPNMYsBbDWZhhjak3boDFmGDAMICnp2Oc/F/Gp1MXOnT8GbvwaWpxU5ZfYkbuDkfNG8r+t/6N7k+6MPHUkbRu0rfLriEjN5k0BUOx5Xm8BjDFNcFoEjtVWDnytcEvPuvL2STXGhACxwG4vjwXAWjseGA+QnJxsqyBuEd/4czp8fiNEN4VrJ0Gj46r09G7r5os/v+DlxS/jtm4eOPkBrupwFcFH8NpfEak7vBkFMAaYDDQ1xjwL/Az8swqu/SvQzhjTxtOicCUw9aB9pgI3eL5fBsyy1lrP+is9owTaAO2AY+2TIOI/Sz6ET66Cxu3h5hlVnvxT9qZwy/e38PT8p+nSqAtfXvwl13a6VslfJIB5Mw/Af4wxi4H+gAH+Yq1dfawX9jzTvxOYDgQD71prVxpjRgKLrLVTcSYh+tDTyW8PTpGAZ7/PcDoMlgB/0wgAqZWshTkvwI/PwnH9nWF+4VU3057L7eI/q//Dv5f+m5CgEJ485UkubXepXtkrIl6NAhiD0+P+9aq+uLX2G+Cbg9Y9XuZ7AXB5Bcc+Czxb1TGJVBu3C6b9AxZPgO5XwcX/huCq64S3PnM9j899nGU7l3FmyzN5tM+jNItqVmXnF5HazZs+AIuBR40xJ+A8CvjUWrvIt2GJ1HHF+fDFzfDHNDhtOPR/vMrG+Be7i5mwYgLjfh9HVGgUo04fxfltztddv4gcwJtHAO8D7xtjGgJ/BUYbY5Kste18Hp1IXZS3Bz65ElIWwnkvQO9hVXbq1btX8/jcx1mzZw3ntj6Xh3o9RKOIRlV2fhGpO45kJsDjgQ5AK+CY+wCIBKTMLfDRXyFjM1z+HnT+S5WcttBVyJu/v8m7K94lrl4cr/Z7lf5J/avk3CJSN3nTB+B54BJgPTAReNpam+nrwETqnB3L4aPLnOb/6yZD675Vctoc3Fz+38vZmLWRwccNZsTJI4gNj62Sc1eJZl39HYGIlMObFoD1wCnW2l2+DkakztrwE0y8FsJj4KbvIL7TMZ2uyFXErJRZ/EkRe7EklBQwbsA4+raomqJCROq+CgsAY0wHa+0anPH6SZ53AJSy1i7xdXAidcLyL2DybdDoeLj2C4htedSn+mPPH0xaO4lpG6eRVZhFGNCcYCYNnkxUaFTVxSwidV5lLQDDcabQfamcbRbn3QAiUpl5r8P0hyHpVLjqY4iIO+JTZBVm8e3Gb5m8bjKrdq8iNCiU/kn9uaTdJYz//u8YjJK/iByxCgsAa+2+rsnnecbjlzLG1PNpVCK1ndsNMx6Dea9Bx4vh0rcg1Pv/bNzWzcIdC5m8djIzt8yk0FVIh4YdeKjXQ1zQ9oLSZ/xvUfOH9k0YNMHfIYhIObzpAzAXONGLdSICUFIEX90OK76AXsNg0Cjwcsrd7Tnb+Wr9V0xZN4WtOVuJCYvhkuMv4dJ2l9KxUUcfBy4igaSyPgDNcF6xG2GM6Qmltxr1gchqiE2k9inY63T22/gT9H8CTrv3sBP8FLmKmLVlFpPXTWbetnlYLH0S+nBXz7vo36o/4cHh1RS8iASSyloAzgVuxHnT3kvsLwD2Ag/7NiyRWih7hzPMb+dq+Ms46HFVpbuv2bOGyWsnl3boS4hK4LbutzH4+MG0iG5RTUGLSKCqrA/AvhkA/2qt/bIaYxKpfXathQ8vhbzdcPVEOH5AubtlFWbxzcZvmLx2Mqv3rD6gQ1/vZr31dj4RqTbe9AE4yRgzc9/kP8aYOOAf1tpHfRuaSC2RshA+HgJBIXDj19DiwO4x+zr0TVo7iZmbZ1LkLiq3Q5+ISHXypgA4z1pb2uRvrc0wxpwPqAAQ+eNb+Hwo1E+Aa7+Ehm1LN5XXoe/SdpeqQ5+I1AjeFADBxphwa20hgDEmAlCvJJHF78HX90JCD7j6M4huQqGrkB+3/MiktZOYv32+OvSJSI3lTQHwH2CmMWbfYN6hwPu+C0mkhrMWZo+Cn0bB8efA5e+xJjeVyQve5usNX7O3aK869FQZwTgAACAASURBVIlIjefN64BHG2N+B/b1anraWjvdt2GJ1FCuEpg2HJa8T1b3K/jmhDOY/P1NpR36BiQN4C/t/kKfhD4EmSB/RysiUiFvXwe8Giix1v5gjIk0xsRYa7N9GZhIjVOUh/vzoSxI+ZHJnU5jZs5iin6d598OfXrTnogcJW9eB/x/OO8EaAgchzM50DhALxuXgLEtfSVTvr6Zr9xZbEuIJ8aVoQ59IlKredMC8DegF7AAwFq71hjT1KdRidQQmbjIsCUM+uZKbDD0ievAPT2GcXbS2erQJyK1mjcFQKG1tsh4pjM1xoTgvA1Q5NhMuMD5HDrNv3GUo9hdzMuLXmadKSHeVcJt+S4GD3iJFh0u8ndoIiJVwpsC4CdjzMM47wQ4B7gD+K9vwxLxn135u/jH7H+wJH0JV+3N5ubMbOJv/QWadvB3aCIiVcabbsoPAjuB5cCtwDdoEiCpo5amL2XIlEtZnbaU0em7OD+vkOcSEpX8RaTO8WYYoBt4y/MnUidZa/l49X948dcXaF5czLiMXNoPeIGhv71y2Lf5iYjURpW9Dvgza+0QY8xyDn3mb4E9wKvW2im+DFDE1/JL8nnqpweYlvojZ+Xm8Wx0Z+oPewNiW8Lvr/o7PBERn6isBeBuz+eFFWxvjDNLoAoAqbW2ZG3m3u+GsjY/nb/vzeeWUx4mKPkm3fWLSJ1X2euAt3s+NxtjmuEMBbTAr9baHcBmY8w11ROmSNX76Y9JPDT/KYJcxYwNSqDvde9AXGt/hyUiUi0O2wnQGHMLsBC4FLgMmG+MuQnAWrvYt+GJVD2Xq4TXv7udO+c/QcuiQj5tN5S+189Q8heRgOLNMMARQE9r7W4AY0wjYC7wri8Dk7pvqEkDYMJh9qtKWXvW8cDX1/GLzWGwO4JHB0+kXtNO1RiBiEjN4E0BsBsoO+9/tmedSK2yeuHr3Lv8DdKCDY81PY3Lz30NE+zt6zBEROqWykYBDPd8XQcsMMZMwekDMBhYVg2xiVSN3N1MnXojIws2EBscwvt9n6FbO83oJyKBrbLbnxjP53rP3z7q9S+1RvHKrxg95yEmRoZwcmRzXrjwPzSK0qssREQqGwXwVNllY0y0Z33OsV7UGNMQmAi0BjYBQ6y1GQft0wMYC9QHXMCz1tqJnm3vAWcCWZ7db7TW/nascUkdkp9B2rR7GL57Hssiwxna+kLuOv1pQoLU5C8iAt69DrgL8CHO64AxxuwCrrfWrjyG6z4IzLTWjjLGPOhZfuCgffI811lrjGkOLDbGTLfWZnq2j7DWfnEMMUhdtXYGv35zF/fFGAoionjptGcZ2PZ8f0flExMGVWcXShGpS7y5HRoPDLfW/ghgjDkLZ1rgU4/huoOBszzf3wdmc1ABYK39s8z3bcaYdKAJkIlIeQr2Yr97iA82TOGVhnEkRiUw4ZxxtG3Q1t+RiYjUON68DChqX/IHsNbOBqKO8brx+yYaAnYA8ZXtbIzpBYRxYF+EZ40xy4wxrxhj9GL2QLf+R/LeOIUR277jxUZx9EvqxyeDJyn5i4hUwJsWgA3GmMdwHgMAXAtsONxBxpgfgGblbHqk7IK11hpjDn7XQNnzJHiufYPnxUQAD+EUDmE4LRQPACMrOH4YMAwgKSnpcGFLbVOYAzMeZ+Pv73NvQgs2Bkdxz4l3c1OXmzCazldEpELeFAA3AU8Bk3CGAf7Ps65S1toBFW0zxqQZYxKstds9CT69gv3qA9OAR6y188uce1/rQaExZgJwXyVxjMcpEkhOTq6w0JBaaNMvMOUOZham80hiEmFh0bx55gv0Sejj78hERGo8b14HnAHcVcXXnQrcAIzyfB4ytNAYEwZMBj44uLNfmeLBAH8BVlRxfFKTFeXBrKdxzR/LvxOSeKdBY7o07MDLZ71MQnSCv6MTEakV/DUmahTwmTHmZmAzMATAGJMM3GatvcWz7gygkTHmRs9x+4b7/ccY0wQwwG/AbdUcv/hLykL46nYyMjZwf/sezC/ezWXtL+PBXg8SHqyuICIi3vJLAeB5r0D/ctYvAm7xfP8I+KiC48/2aYBS8xQXwOx/wtx/syKuBcPbd2V3cTYjTx3JJe0u8Xd0IiK1jjdvA+zrzToRn9m6BMafCb/8iy87nc31cWEQUo8PzvtAyV9E5Ch5Mwzw316uE6laJUUw6xl4ewCFBXt5svcQnsz7k+T4ZCZeOJHOjTv7O0IRkVqrspcBnYIz2U+TMi8GAmdq3mBfByYBbsdymHw7pC1ne9dLuTc0h5Xp8/m/rv/H33r8jeAg/SsoInIsKusDEAZEe/aJKbN+L3CZL4OSAOYqgZ9fgZ9GQ0Qc884byf0bv6CksIRX+71K/6RDuo6IiMhRqOxlQD8BPxlj3rPWbgYwxgQB0dbavdUVoASQ9DXw1W2wbSm286W8c9yJ/HvF27Sp34ZX+71K69jW/o5QRKTO8KYPwHPGmPrGmCic8farjDEjfByXBBK3C35+Fd48HTK3kHPpOO5tHMu/lo9nYKuBfHzBx0r+IiJVzJthgJ2stXuNMdcA3+K8uW8x8IJPI5OAEF9cBO8OgtSF0OFC1p9xN/cseJqU7BRGJI/guk7XaUpfEREf8KYACDXGhOLMuPeatba4srn7RbzVMzeHW3ftgPAsuPRtpsfU57Ef7yQiJIK3Br7Fyc1O9neIIiJ1ljePAN4ENuG8AXCOMaYVTkdAkaO3YwW37trB1tAwSm77hRcLN3PfnPtoH9eezy78TMlfRMTHvHkXwBhgTJlVm40x/XwXklSVod8NBWDCoAl+juQgeXvg06vJCwrihaYJFMx/jEVpi7jyhCu5/+T7CQ0O9XeEpSbYSt9ULSJSax22ADDGxAP/BJpba88zxnQCTgHe8XVwUge5SuDzGyB7O0/GN2dBiJugXcv552n/5KLjLvJ3dCIiAcObRwDvAdOB5p7lP4F7fBWQ1HEzHsNunMNHfa7hxzCDAT46/yMlfxGRauZNAdDYWvsZ4Aaw1pYALp9GJXXTb5+Qu2AsIzr0ZvS2mcQSRCfC6NCwg78jExEJON4UALnGmEaABTDG9AGyfBqV1D2pi1n33XCubNWGGUVp3HPiPRxHCCFoiJ9IXXDFm/O44s15/g6jQjU9Pqj+GL0pAIYDU4HjjDG/AB8Af/dpVFK3ZKfx36+u4+pmjcmuF8PbA9/m5q43Y5T8RUT8xptRAEuMMWcCJwAG+MNaW+zzyKROKCrMZvQXF/NZTDAnxXXghQFv0CSyib/DEhEJeN6MAqgH3AGchvMY4H/GmHHW2gJfBye129bsVP4xZQgrg/IYGt+Xuwa+RkiQN3NP1SBDp/k7glpvX5PmxFtP8XMkFavpMdb0+KR28ub/jT8AsoF/e5avBj4ELvdVUFL7zUmdw0M/DscW5/Nqk770HzTO3yGJiEgZ3hQAXay1ncos/2iMWeWrgKR2c7ldvP7b67y1/C1OKCrmlYgTSLxgrL/DEhGRg3hTACwxxvSx1s4HMMb0Bhb5NiypjXbn7+aBOQ+wYMcCLskv4eHiSOpd9wEEBfs7NBEROUiFBYAxZjnOM/9QYK4xZotnuRWwpnrCk9piafpS7pt9H1lFWYwsjuSSjG1wyxSoF+vv0EREpByVtQBcWG1RSK1lreXDVR/yyuJXSIhO4CPa0WHdt3DVp9Ckvb/DExGRClRYAFhrN1dnIFL75BTl8Pjcx5mxeQZnJ57NMyGJxMx6Gs5+FE4Y5O/wRESkErVsTJbUFH9m/Mnw2cNJzU7lHyf9gxvCW2A+HgKdBsPp9/k7vCpT04df1fT4RKTmUgEgR2zq+qk8Pe9posOieXvg2ySHxsFb/aBJRxj8BhjN8CciUtOpABCvFboKGbVwFF/8+QUnNzuZ5894nsZB4fD2ADBBcNXHEB7t9flyi0p8GK2IiFRGBYB4JTU7leGzh7N6z2pu7nIzd/a8kxCC4LPrYNdauG4SxLX2d5giIuIlFQByWLNTZvPwzw+DhTH9xtAvqZ9nwyhY8zWc+xy0PcufIYqIyBFSASAVKnGX8NrS13hnxTt0bNiRl856icSYRGfj6q9h9nPQ/Wroc7t/AxURkSOmAkDKtSt/Fw/MeYCFOxby13Z/5aHeDxEeHO5sTF8Nk2+F5ifCha+o05+ISC2kAkAOsThtMSN+GkF2UTbP9H2GwccP3r8xPwM+vRpCI+GKjyC0nv8CFRGRo6YCoC7bsfyIdrfW8sGqD3hl8Su0jGnJ2AFjOaHhCft3cLvgi5sgMwVunAaxLao4YBERqS4qAASA7KJsHvvlMWZumcmApAGM7DuSmLCYA3f64UlYPwsu+hck9fZLnCIiUjX8UgAYYxoCE4HWwCZgiLU2o5z9XMC+29gt1tqLPevbAJ8CjYDFwHXW2iLfR143/bHnD4bPHs7WnK3cl3wf13e6HnPwc/1ln8PcMZB8M5x0o1/iFBGRqhPkp+s+CMy01rYDZnqWy5Nvre3h+bu4zPrRwCvW2uOBDOBm34Zbd3217iuu+eYaCkoKePfcd7mh8w2HJv9tv8HUOyHpVBg0yj+BiohIlfLXI4DBwFme7+8Ds4EHvDnQONnpbODqMsc/CYytygDruoKSAp5b+ByT1k6id7PejD5jNI0iGh26Y85O+PQaiGwMQz6AkLAqi+GJXd7PGigiIlXLXwVAvLV2u+f7DiC+gv3qGWMWASXAKGvtVzjN/pnW2n3zyKYCFfZGM8YMA4YBJCUlVUXstV7K3hSG/zScNXvW8H9d/4+/9fgbwUHBh+7oKobPb4C8XXDTdxDdpPqDFRERn/BZAWCM+QFoVs6mR8ouWGutMcZWcJpW1tqtxpi2wCxjzHIg60jisNaOB8YDJCcnV3SdOqm8ufZnbZnFoz8/ijGG1/u/zhktz6j4BN89BJt/gUvfhuY9qzy+kY1eAJzOICIiUr18VgBYawdUtM0Yk2aMSbDWbjfGJADpFZxjq+dzgzFmNtAT+BJoYIwJ8bQCtAS2VvkPqGNK3CWMWTqGCSsm0KlRJ14+62VaRFcyjG/JB/DrW3Dq36Hb5dUXqIiIVAt/PQKYCtwAjPJ8Tjl4B2NMHJBnrS00xjQG+gLPe1oMfgQuwxkJUO7xst+u/F2M+GkEi9IWMaT9EO7vdf/+Wf3Kk7IQvh4Ox50NA56qvkBFRI5BUYmbzLwi9uQVsSe3iIzcYvbkFZGRW8Sm3bm43Jbhn/3m7zArtH5nDlFh1ZeW/VUAjAI+M8bcDGwGhgAYY5KB26y1twAdgTeNMW6c0QqjrLWrPMc/AHxqjHkGWAq8U90/oLbINZbL/3s5OUU5/PO0f3LRcRdVfsDebTDxWohtCZe9C+X1DRAR8bESl5vM/GIycj3JPK+IPbnFnk8nqe9L7s5nMTmFFb9iPDjIEGwMCzfuqcZfcWSyC0oIqsap1f1SAFhrdwP9y1m/CLjF830u0LWC4zcAvXwZY21nrWV3sJv0YEvr0GjGnzOednHtKj+ouMBJ/kW5cP0UiIirnmBFpE5zuy1Z+fvvxg9O6Bmly0Vk5BWzJ7eIrPziCs8XFRZMXFQYcZFhxEWF0aZxFHFRYTT0LDf0bGsYFUZcVCgNIsK47p0FAEy89ZTq+tlH7Io351Xr9TQTYB1kreWlRS+RHmKJcRk+vfBTokKjDncQTBsOWxfDFf+Bph2rJ1gRqVWKXW6y8ovJzHOSdWaek8S3Z+VT7LI8+OWyMgl+3z5FuCvogh0WEkSjMgm7RVwkDSNDD03kns8GkaHUC1XLZFVQAVDHuK2bUQtH8cmaT4grMcS7zOGTP8CCN+G3/8CZD0LHC30fqIj4ldttyS4sKZPIi0qTeUZeMVmez4w85248I6+IzNxisitpZjfArDXppQm7Q7P6xEWFHnBn3iBy3516KA2jwogIDT508jGpFioA6hC3dTNy3ki+XPslN3a+kflLPsDgxX9YG36C6Q9DhwvhTK/mYxKRGsJaS0Gx25O4i8jKKy5N3PuTuifB70vkecVk5Rfjqui2HKhfL4S4fQk7Koy2jaNoEOkk9gaRoTSIDHWa4D3L90z8jWADn912ajX+ejkWKgDqCJfbxeNzH2fq+qkM6zaMO3vcyRVLPjz8gRmb4PMboXE7uGQcBPlrdmipi7Lyi9mZXUhhiYtXZvzp73AqlJqRB1BjY0zNyMNt4aFJy8jILSYz/8C79aISd4XHRoQGExcZ6iTvqFA6Nqtfmrwb7Fvv+dy3PjYilOCgI7srDznC/cX/VAAcpSHjewDw2TD/DykpdhfzyP8e4dtN33Jnjzu5tfut3h1YlOtM82tdcOXHEB5z+GNEDmNvQTE/rEpj2rLtzFm7k2KXc5f5r5lr/RzZ4dXkGA0wY1U6cZ4kndgwkm4tY52EXXo3fmgi1/NyqYgKgFqu2FXM/XPu54ctPzD8pOEM7TLUuwOtha/ugPRVcM3n0Og43wYqddq+pP/N8u3M+XMXRS43zWPrccMprZm3fjdR4cE1uml4X+/rmtpDvKbHJ7WTCoBarNBVyPDZw5mTOocHez3INR2v8f7gn1+GVV/BOSPh+AonbRSpUHZBMT+s9tzpl0n615/SivO7JdCjZQOCgky1D20SEe+oAKil8kvyuefHe5i7bS6P9XmMIScM8f7gP6fDzKehy2Vw6l2+C1LqnOyCYmauTudrT/N+UYmbhNh6XHdKK87vmkDPRCfpi0jNpwKgFsorzuPvs/7Orzt+ZeSpI7mk3SXeH7xrLXx5CzTrChf/GzT8Rg5jX9Kftnw7P/25P+lf27sVF3RT0heprVQA1DI5RTncMfMOlu1cxnOnP8cFbS/w/uCCLPjkKggOczr9hUX6LlCp1XIKS5i5Oo2vl+1P+s3q70v6zeiZGKekL1LLqQCoRbIKs7j9h9tZvXs1z5/xPANbD/T+YLcbJg2DjI1w/VRokOi7QKVW2pf0py3bzmxP0o+vH841vZO4oGsCJyYp6YvUJSoAaonMgkyGzRjGusx1vHzWy/RL6ndkJ/jxWfjzOzj/RWjd1zdBSq2TW1jCzDXpTFu2jdl/7KSwxE3TmHCu7pXEhd2U9EXqMhUAtcCu/F0MmzGMLXu3MObsMZzW4rQjO8HKr+B/L8KJ18PJt/gmSKk19iX9b5Zt58c/0kuT/lW9krigWwInKemLBAQVADVcel46t3x/Cztyd/Ba/9fok9DniI5PKi6Cr26Hlr2cu391+gtIuYUlzFqTzrRykv75XRNIbqWkLxJoVADUYDtyd3Dz9JvZlb+LsQPGclL8SUd0fLTbxYg9aRDZBK74EELCfRSp1ER5RQcm/YJiN01iwrny5EQn6bdueMTTvYpI3aECoIZKzU7llu9vYW/hXsYPHE/3Jt2P7AQ56fxjTzpxLpfzet+YZr4JVGqUfUn/m+XbmbVmf9IfkpzIBUr6IlKGCoAaaPPezdw8/WbyS/J569y36Nyos/cHlxTBgnHw0/OcUFTAGw2acFfLI2s5kNrF5bZMW7adb5ZvZ+aaNAqK3TSOdpL++V0TOFlJX0TKoQKghtmQuYFbvr+FEncJ7577Lic0PMH7g//8HqY/BLvXQbtz+UfWSraHhKK5/ry3O6eQ5VuzWLE1i+Vbs1iakonLben25HR/h1au3CIXbrdl0eYMGkeHcflJTtLv1UZJX0QqpwKgBvljzx8MmzGMIBPEhEETOK6Bly/o2bXOSfxrv4dGx8M1X0C7c9jueWOhlG9PbtH+ZJ/qJPytmfml21s3iiQ6PITQYMN5XRL8GGnFvl2xnSBjeHlIDyV9ETkiKgBqiFW7VzFsxjDCg8N5Z+A7tI5tffiDCvbCnOdh/jgIjYCBz0CvWyEkzOfx1jYZnmS/vJJk3zOpAdef0oquLWPp3DyW2IjQ0hfZPHnxETyGqUart+8F4JTjGvk5EhGpbVQA1ADLdi7jth9uIyY0hrfPfZvEmMPM0ud2w2//gZlPQe4u6Hkt9H8coptWT8A1XGbeock+NWN/sm/VKJIeSQ247pRWdGsRS+cWTrIXEQkkKgD8bEnaEu6YeQcN6zXk7YFv0zy6eeUHpCyEb++HbUudsf1XfwYtTqyeYGugrLzi/cl+aybLt2aRsmd/sk9qGEn3lg24tk8ruraIpUvzWGIjlexFRFQA+NHC7Qu5c9adxEfG8/bAt4mPiq94573b4YcnYNlEiEmAS9+CrpcH1MQ+WXnFrNh24J39lj15pdsTG0bQtUUsV/fyJPsW9WkQqcchIiLlUQHgJ79s/YW7f7ybxJhE3hr4Fo0jGpe/Y3EBzH8d5rwE7hI4/R9w2nAIj67egKtZVn4xKz139ss8HfU2796f7FvGOcn+yl6JpXf2cVFK9hKYiouLSU1NpaCgwG8x/K1nBACrV6/2WwyVqenxwbHFWK9ePVq2bEloqPctnCoA/OCnlJ+4d/a9HNfgOMafM564enGH7mQtrJkG3z8CGZugw4VOJ7+Gbao9Xl/bW1DMCk+SX5bqfG4qk+xbNHCS/ZBkJ9l3baFkL1JWamoqMTExtG7dGuOnVsGwnTkAHNekZt6c1PT44OhjtNaye/duUlNTadPG+xyhAqCa/bD5B0b8NIITGp7Am+e8SWx47KE7pa+B7x6EDT9Ck45w3Vdw3BG+/a+GKipxs3r7XpZuyWBdeg65hSV0e/L70u37kv3lyYl08ST7hkr2IpUqKCjwa/KvDWpy4t/naGM0xtCoUSN27tx5RMepAKhG3278lof+9xBdGndh7ICxxITFHLhDfgbMHgUL33Ka+M97HpJvhuDa+Y/JWktqRj5LUzL5bUsmS1MyWLltL0UlbgBCgw3R4SHcefbxpcm+UbTeVyByNJT8A9vR/POvnZmlFpqybgqPz32cE5ueyGv9XyMqNGr/RrcLlrwPM5+Ggkw46Ubo9yhE1a6x3dkFxSxLzeK3lEyWbsngt5RMduUUAVAvNIiuLWK54ZRW9EyKo0diA+75dCnGGO48u52fIxeRY5GZmclH707g2pv+zyfnf/LJJ4mOjua+++47pvO8+uqrDBs2jMjISK+Pad26NYsWLaJx4wP7aY0bN47IyEiuv/76I4ohMzOTjz/+mDvuuAOAbdu2cdddd/HFF18c0XmqggqAavDFn18wct5Ieif0ZszZY4gIidi/cdMv8O0DkLYcWvWF80ZDs67+C9ZLLrflz7Rsfitzd782PQdrne1tm0RxRvsm9EyKo2diA05oFkNocNAB59Adi0jdkJmZyX/ee6vcAqCkpISQkJqRal599VWuvfbaIyoAKnLbbbcd1XGZmZm88cYbpQVA8+bN/ZL8AYIOv4sci49Xf8xT857itBan8Vr/1/Yn/8wU+PxGeO98p+n/sglw47Qam/zT9xYwfeUORn+3hivHz6Pbk9M571//46FJy5m+agfNG0RwT//2vH9TL35/fCCz/nEWLw/pwXV9WtGlRewhyV9E6o4HH3yQLZs2clG/UxkxYgSzZ8/m9NNP5+KLL6ZTp05s2rSJLl26lO7/4osv8uSTTwKwfv16Bg0axEknncTpp5/OmjVryr3G77//zimnnEK7du146623Ste/8MILnHzyyXTr1o0nnngCgNzcXC644AK6d+9Oly5dmDhxImPGjGHbtm3069ePfv0O7VM1c+ZMevbsSdeuXbnpppsoLCws3fb888/TtWtXevXqxbp16wCnVeLFF1+s9DekpaVxySWX0L17d7p3787cuXN58MEHWb9+PT169GDEiBEH/G/Tp08fVq5cWXrds846i0WLFpGbm8tNN91Er1696NmzJ1OmTDnif0blqRllWR31/sr3eXHRi5ydeDYvnPkCYcFhUJQHc8fAz68CFs56CE69C8KOvSKtKgXFLlZs3deUn8lvKZml0+aGBBk6Na/PZSe1pEdSA3omxtGqUaTu5kVqCF/9t2j3Ne+VY9SoUSz5fRn//XEuxzWJZvbs2SxZsoQVK1bQpk0bNm3aVOGxw4YNY9y4cbRr144FCxZwxx13MGvWrEP2W7ZsGfPnzyc3N5eePXtywQUXsGLFCtauXcvChQux1nLxxRczZ84cdu7cSfPmzZk2bRoAWVlZxMbG8vLLL/Pjjz8e0pxfUFDAjTfeyMyZM2nfvj3XX389Y8eO5Z577gEgNjaW5cuX88EHH3DPPffw9ddfe/Ub7rrrLs4880wmT56My+UiJyeHUaNGsWLFCn777TeAA/63ueKKK/jss8946qmn2L59O9u3byc5OZmHH36Ys88+m3fffZfMzEx69erFgAEDiIqK4lioAPCR8cvG8++l/+bc1ufy3OnPEWpCYOVk+P4xyEqBzpfAOU9Dg8NM++tj1lo27so9INmv3r6XErfzH3uLBhH0TGrA0L6t6ZkUR+fm9akXGuzXmEWk5uvVq9dhh6Tl5OQwd+5cLr/88tJ1Ze+8yxo8eDARERFERETQr18/Fi5c+P/t3Xt4FtW1+PHvSriEmxAQUBKQ2COkhltCQJCrUlEKVqEocqQajhw8HrEq/qhW5Kboo8BR26ogVQELCFgLYoEjXqCCWkAgIBBBfjW2gIrGgCD3ZJ0/Zr8vk5A7Sd7ArM/z5GEue2bW7Bne2bNnz2zWrl3LypUrSU5ODq/v888/p0ePHjzwwAM8+OCDDBgwgB49ehQZx86dO0lISKBVq1YA3H777Tz//PPhAsDQoUPD/95///0l3of333+fV199FYDo6Gjq169PdnZ2oXHcfPPN9O3bl0mTJrFo0SIGDx4MwMqVK1m6dGm4xuHYsWP885//5Kc//WmR+1WciBQARKQhsBBoCWQCN6tqdr40VwHP+CYlAreo6hIRmQ30Ag66eWmqml7BYZeIqvJ8+vO8uPVFBlw6gMe6PUa1/Rmw4iH4ci00bQsDZ0DL7hGJ78CRE3ku9lv2HODAkZMA1KkRTfvmDRjZ89JwQ73G9axVvjHnkqLu1CtSjeioPK+x+e9Oq1WrRm5ubng8WWJ3hgAAGXtJREFU9MGi3NxcGjRoEL4bLkr+mg0RQVX57W9/y5133nlG+k2bNrF8+XIeeeQR+vTpw/jx40u9TwVtO38cpdmH4sTFxdGoUSO2bt3KwoULmTFjBuAd0zfeeIPWrUvRPXwJROrB7EPAe6p6GfCeG89DVVepagdV7QBcDRwBVvqSjAnNr0oX/2c2PcOLW19k0GWDmJx8P9WWj4EXe8L+HdD/abjzb5V28c/VKH483oxXP85k9MJ0rp62mg6PvkParA38/v3P+eaHY1yXdBFP/bItb9/Xk60Tr2X+f3bhN9clcs3lTe3ib4wpkXr16nHo0KFC5zdt2pT9+/eTlZXF8ePHw1XoF1xwAQkJCbz++uuA9xu6ZcuWAtfx5ptvcuzYMbKysli9ejWdOnXi2muv5ZVXXuHwYe8DOnv37mX//v3s27eP2rVrM2zYMMaMGcOmTZuKjLN169ZkZmaGn+//6U9/olevXuH5CxcuDP/btWvXPMsWtQ99+vRh+vTpAOTk5HDw4MFi82rIkCFMmTKFgwcP0q5dOwCuvfZa/vCHP4QLd5s3by50+dKI1COAG4DebngOsBp4sIj0g4EVqnqkiDQRpapM2TCFuRlzGdLqJh6WJkQ9lwrHD0PnkdD7IahVwBf/zsLJnFy+PniMPdlH2ZN9hL0HjrIn+yh7s4+y58AR9n0/kRyiGb93O43r1aRD8wYMTo2nQ/MGtItvQN2a9gTIVLyFd3YtPlGEVfUYq3p8jRo1olu3brRp04Z+/frRv3//PPOrV6/O+PHj6dy5M3FxcSQmJobnzZs3j7vuuovJkydz8uRJbrnlFtq3b3/GNtq1a8dVV13Fd999x7hx42jWrBnNmjUjIyMjfFGuW7cuc+fOZffu3YwZM4aoqCiqV68evgiPHDmS6667jmbNmrFq1arwumNiYpg1axY33XQTp06dolOnTnla+WdnZ9OuXTtq1qzJa6+9Fp4eqg0obB9+97vfMXLkSF5++WWio6OZPn06Xbt2zZNXd999d579HDx4MPfeey/jxo0LTxs3bhz33Xcf7dq1Izc3l4SEhDPaIZSFRKK6SEQOqGoDNyxAdmi8kPTvA0+r6l/d+GygK3AcV4OgqgU+OBKRkcBIgBYtWnT88ssvy2Ufbp7ZAYBFI9PJ1Vwe//vjLNq1iGHNevObXeuRbz+DS3vDdU9Ck7I9pzl+Kod9B455F/T8F/jsI3z9wzFyfYdPBJrWiyEuthbxsbWouePPXBr9DQN+/XviGtSqcg31hrz4MVC1f9yqeoxVPT5TOTIyMs76ebApnXvuuYeUlBSGDx8e6VDCCjoPRGSjqqYWlL7CbgFF5F3gogJmjfWPqKqKSKGlEBG5GGgLvO2b/Fvga6AGMBOv9uDRgpZX1ZkuDampqeVe2snJzWHixxNZsnsJd0Q35t4PX0UaXAJD5kFi/yJ76zt6Ioe9B464O/ijvgu8N23/obxlmiiBi+vXIi62Fl0ubUR8bC3iY2uHL/gX1Y+hZrXTDfS2P3EPAPGxVecNA2OMOdeNGzeOdevWhV9lPFdVWAFAVX9W2DwR+UZELlbVr9wFfn8Rq7oZWKyqJ33r/soNHheRWcDZfR6qjBTl4Q9+w/IvV3LXwcPcdSgLuXocdB0F1WM4dOwkew+E7thDF/gj4fGsH0/kWV/1aOHi+t7FvFerxnku7nENvAu8vU9vjDGR9dhjj/HYY49FOoyzFqmHwEuB24En3b9FfdVgKN4df5iv8CDAjcC2igq0MLm5uRyPymH5lyu59/sD9K7Vg2cTR5KRWZe96RvYk32Ug0dP5lmmRrUo4ht4d/B9m11AXAPvDj4+1pvWpF4M0VFVq5reGGPM+SlSBYAngUUicgfwJd5dPiKSCvyXqo5w4y2B5sDf8i0/T0QaAwKkA2X7JuNZqJ0Tzc5ayrDvhOXfjWaytqLW3mPExwpxsd6783ENTl/c42NrcWGdmkTZBd4YY0wVEJECgKpmAX0KmP4JMMI3ngnEFZDu6oqMryRiTtSjU3Yuba9ZSL+GdYiPrU1s7epVrqGdMcYYUxB7D6yMDtT5AYCftzujfGKMMVWevUFirEWZMcaYKqdly5Z89913kQ7jvGYFAGOMMeVKVfN8+tdUTVYAOI892mgqjzaaGukwjDEBkJmZSevWrbntttto06YNd9xxB6mpqSQlJYW76QXvzn7ChAmkpKTQtm3bcNe5WVlZ9O3bl6SkJEaMGJGnT4Onn36aNm3a0KZNG5599tnw9hITE0lLS6NVq1bceuutvPvuu3Tr1o3LLruM9evXV24GnIOsDYAxxpxHJr21nR37fig23Y6vvDShtgBFubzZBUy4PqnYdJ9//jlz5syhS5cufP/99zRs2JCcnBz69OnD1q1bw9+2v/DCC9m0aRMvvPAC06ZN46WXXmLSpEl0796d8ePHs2zZMl5++WUANm7cyKxZs1i3bh2qyhVXXEGvXr2IjY1l9+7dvP7667zyyit06tSJ+fPns3btWpYuXcoTTzzBkiVLio05yKwGwBhjTLm45JJL6NKlCwCLFi0iJSWF5ORktm/fzo4dO8LpBg0aBEDHjh3JzMwE4IMPPmDYsGEA9O/fn9hYr++UtWvXMnDgQOrUqUPdunUZNGgQa9asASAhIYG2bdsSFRVFUlISffr0QURo27ZteL2mcFYDYIwx55GS3KlDxbwFEOoC+IsvvmDatGls2LCB2NhY0tLSwl0AA9Ss6fU0Gh0dzalTp8q8vdB6AKKiosLjUVFRZ7XeoLAaAGOMMeXqhx9+oE6dOtSvX59vvvmGFStWFLtMz549mT9/PgArVqwgOzsbgB49erBkyRKOHDnCjz/+yOLFi+nRo0eFxh8UVgNgjDGmXLVv357k5GQSExNp3rw53bp1K3aZCRMmMHToUJKSkrjyyitp0aIFACkpKaSlpdG5c2cARowYQXJyslXxlwMrABhjjDlrLVu2ZNu2092yzJ49u8B0/gt3amoqq1evBqBRo0asXLmywGVGjx7N6NGjS7y9/PNMwawAYIwxAWRfADTWBsAYY4wJICsAGGOMMQFkBQBjjDEmgKwNgDFFqOrPSat6fMaYqstqAIwxJohm9ff+TGBZAcAYY0yFSUtLIyEhgQ4dOtChQweuvPLK8LwVK1aQmprK5ZdfTnJyMg888AAAEydOZNq0aZEKOTDsEYAxxpgKNXXqVAYPHpxn2rZt2xg1ahTLli0jMTGRnJwcZs6cGaEIg8kKAMYYY8rF448/zpw5c2jSpAnNmzenY8eOhaadMmUKY8eOJTExEfD6BbjrrrsqK1SDFQCMMeb8suIh+PrT4tN9vdX7tyTtAC5qC/2eLDLJxo0bWbBgAenp6Zw6dYqUlJRwAWDMmDFMnjwZgKSkJObNm8e2bdvCVf4mMqwAYIwx5qytWbOGgQMHUrt2bQB+8YtfhOcV9AjARJ4VAIwx5nxSzJ16WOjOf/iyioulCElJSWzcuJH27dtHZPvG3gIwxhhTDnr27MmSJUs4evQohw4d4q233ioy/ZgxY3jiiSfYtWsXALm5ucyYMaMyQjWO1QAYY4w5aykpKQwZMoT27dvTpEkTOnXqFJ7nbwMAsH79etq1a8ezzz7L0KFDOXLkCCLCgAEDIhF6YFkBwBhjTLkYO3YsY8eOBbx3+aHwboEBBgwYUOBFP7SsqVhWADDGmCCK0LN/U3VYAcAYY0y5s7v4qs8aARpjjDEBZDUAJmKsJztjjIkcqwEwxhhjAsgKAMYYE0DD/3c4w/93eKTDMBFkBQBjjDEVprDugGfPns2oUaMiHF2wRaQAICI3ich2EckVkdQi0l0nIjtFZLeIPOSbniAi69z0hSJSo3IiN8YYU1pTp04lPT2d9PR0Pvroo0iHY5xINQLcBgwCXiwsgYhEA88D1wB7gA0islRVdwBPAc+o6gIRmQHcAUyv+LDPLdbIzhhTmUrTHbCJvIgUAFQ1A0BEikrWGditqv9waRcAN4hIBnA18O8u3RxgIlYAMMYYnlr/FJ99/1mx6UJpStIOILFhIg92frDINKXtDthEXlV+DTAO+JdvfA9wBdAIOKCqp3zT4yo5NhaNTK/sTRpjTJVl3QGfeyqsACAi7wIXFTBrrKq+WVHbLSCOkcBIgBYtWlTWZo0xJiKKu1MPCd35z7puVkWGY6qwCmsEqKo/U9U2BfyV9OK/F2juG49307KABiJSLd/0wuKYqaqpqprauHHjsuyKMcaYYpS2O2ATeVX5NcANwGWuxX8N4BZgqaoqsAoI1SfdDlRajYIxxpgz+bsD7tev3xndAYdeA+zQoQMnTpyIYKQmJFKvAQ4UkT1AV2CZiLztpjcTkeUA7hn/KOBtIANYpKrb3SoeBEaLyG68NgEvV/Y+GGOMyWvs2LHs2rWLtWvX0qpVK8B73/+LL74IvwaYnp5OjRo1SEtL47nnnotwxMEWqbcAFgOLC5i+D/i5b3w5sLyAdP/Ae0vAGGNMGdizf1OV3wIwxhhzjrLugKu+qtwGwBhjjDEVxAoAxhhzHvDaR5ugKsvxtwKAMcac42JiYsjKyrJCQECpKllZWcTExJRqOWsDYIwx57j4+Hj27NnDt99+G+lQTITExMQQHx9fqmWsAGCMMee46tWrk5CQEOkwzDnGHgEYY4wxAWQFAGOMMSaArABgjDHGBJAEqdWoiHwLfFmOq7wQ+K4c13eus/w4zfIiL8uPvCw/TrO8yKu88+MSVS2wJ7xAFQDKm4h8oqqpkY6jqrD8OM3yIi/Lj7wsP06zvMirMvPDHgEYY4wxAWQFAGOMMSaArABwdmZGOoAqxvLjNMuLvCw/8rL8OM3yIq9Kyw9rA2CMMcYEkNUAGGOMMQFkBYAyEpHrRGSniOwWkYciHU8kiUimiHwqIuki8kmk46lsIvKKiOwXkW2+aQ1F5B0R+dz9GxvJGCtTIfkxUUT2unMkXUR+HskYK4uINBeRVSKyQ0S2i8i9bnogz48i8iNw54eIxIjIehHZ4vJikpueICLr3LVloYjUqLAY7BFA6YlINLALuAbYA2wAhqrqjogGFiEikgmkqmog3+UVkZ7AYeBVVW3jpk0BvlfVJ10BMVZVH4xknJWlkPyYCBxW1WmRjK2yicjFwMWquklE6gEbgRuBNAJ4fhSRHzcTsPNDRASoo6qHRaQ6sBa4FxgN/EVVF4jIDGCLqk6viBisBqBsOgO7VfUfqnoCWADcEOGYTISo6gfA9/km3wDMccNz8H7kAqGQ/AgkVf1KVTe54UNABhBHQM+PIvIjcNRz2I1Wd38KXA382U2v0HPDCgBlEwf8yze+h4CexI4CK0Vko4iMjHQwVURTVf3KDX8NNI1kMFXEKBHZ6h4RBKLK209EWgLJwDrs/MifHxDA80NEokUkHdgPvAP8f+CAqp5ySSr02mIFAFMeuqtqCtAPuNtVARtHvedsQX/WNh34CdAB+Ar4n8iGU7lEpC7wBnCfqv7gnxfE86OA/Ajk+aGqOaraAYjHq1lOrMztWwGgbPYCzX3j8W5aIKnqXvfvfmAx3okcdN+4552h5577IxxPRKnqN+7HLhf4IwE6R9zz3TeAear6Fzc5sOdHQfkR5PMDQFUPAKuArkADEanmZlXotcUKAGWzAbjMtdasAdwCLI1wTBEhInVcYx5EpA7QF9hW9FKBsBS43Q3fDrwZwVgiLnSxcwYSkHPENfR6GchQ1ad9swJ5fhSWH0E8P0SksYg0cMO18BqVZ+AVBAa7ZBV6bthbAGXkXlN5FogGXlHVxyMcUkSIyKV4d/0A1YD5QcsLEXkN6I3Xi9c3wARgCbAIaIHXA+XNqhqIhnGF5EdvvOpdBTKBO33PwM9bItIdWAN8CuS6yQ/jPfcO3PlRRH4MJWDnh4i0w2vkF413M75IVR91v6kLgIbAZmCYqh6vkBisAGCMMcYEjz0CMMYYYwLICgDGGGNMAFkBwBhjjAkgKwAYY4wxAWQFAGOMMSaArABgAkdEVotIaiVs59cikiEi80qYPk1EnqvouEoQx40icrlv/FER+VkFbq+WiPzNfRa1t4j8taK2VRoicriQ6R+VYNmXQnkoIg+XYfkCt11a/jiKSHNjCdIMEJFHyyMmU3VYAcCYUvB9oask/hu4RlVvrah4ilPKeENuBMIXBFUdr6rvll9UZ/gPvN7PcipwG+VGVa8sQZoRvt5BH843r9jly0u+OAqT53gXYhlwvYjULp/ITFVgBQBTJYlIS3f3/EfXV/ZK97WsPHfwInKh6444dAe9RLz+1TNFZJSIjBaRzSLydxFp6NvEr1y/49tEpLNbvo7riGS9W+YG33qXisj7wHsFxDrarWebiNznps0ALgVWiMj9+dLHiMgsEfnUbecq3+zmbv8+F5EJvriWiddv+DYRGeKmd3R3zhtF5G3fp2VXi8izIvIJMFZEvhSRKN+6/iUi1UXkP0Vkg1vvGyJSW0SuBH4BTHX58xMRmS0ig93yfVzMn7q8qummZ4rIJBHZ5OYluum95HQf75vFfTUyn1vJ+7WzC9z+7hSRGb7Y+4rIx24br4tIXRG5WkSW+PL2GhFZ7IaHuli2ichTvjSHReRxt99/F5GmbnqCW/+nIjK5gDjDy7t/e7u8/rOIfCYi80REfMcgVUSeBGq5/Z+Xb/m6IvKeL8+K7FFUvP8Toe1kuO3WLua4+P+vnLHfhRzvX4vIDvE65lkA4f4KVgMDiorRnGNU1f7sr8r9AS2BU0AHN74I74tY4P0QpbrhC4FMN5wG7AbqAY2Bg8B/uXnP4HU8Elr+j264J7DNDT/h20YDYBdQx613D9CwgDg74n3VrA5QF9gOJLt5mcCFBSzzAN7XI8Hr/OOfQIzbzldAI6AW3udQU4FfhuJ1y9TH6zr0I6CxmzbEt87VwAu+9G8CV/nSveSGG/nSTAbuccOzgcG+ebPxPk0ag9cLZis3/VVfnmb6lv9v3zbeArq54bpAtXx5UQP42jfeGziGV3iKxushbbA7zh/g9Z8O8CAwHhDgM18+zAeuB5q5fG2M94XK94EbXRoFrnfDU4BH3PBS4DY3fDde//QFnZuHfbEexPteexTwMV7HWKFjkOpPX8Dy1YALfOfxbk5/nO2MbeP9n1Bffr4C/L9ijos/jsL2O//x3gfUDP0/8E2/FfhDpH8b7K/8/qwGwFRlX6hquhveiPcDWJxVqnpIVb/F+3F+y03/NN/yr0G47/oLxPsmd1/gIfG651yN98PawqV/Rwv+VGt3YLGq/qhe395/AXoUE2N3YK7b/md4n4Jt5dtOlqoedevq7mK/RkSeEpEeqnoQaA20Ad5x8T6CdyEKWZhveIgbvsU3r42IrBGRT/F+3JOKibs13jHZ5cbn4BWgQkId3fiP1YfA0yLya7yLySnyuhA4kG/aelX9h3qPBF5zedAFr5r6Q7e/twOXqKoCfwKGuWPYFVgBdAJWq+q3bpvzfLGeAELtDPyxdnPbw62zJNar6h71OrFJp2TnaIgAT4jIVuBdvG5fi+sW+F+q+qEbnouXN8Udl5DC9ju/rcA8ERmGVwgP2Y9XsDLnibI8HzSmsvi/f52Dd1cM3o9SqPAaU8Qyub7xXPKe7/m/ga14P8i/VNWd/hkicgXwY6kiL7sz4lLVXSKSAvwcmCwi7+H1v7BdVbsWsh5/vEvxLjQN8Wos3nfTZ+PdFW8RkTS8O9qzEcrrHFxeq+qTIrLMxf6hiFzrCj0hRznzGBZ2bN5R1aEFbHcWXkHvGPC6qp5yNfGFOekKDnliLWTbxcl/jpbmN/VWvBqKjqp6UrxHWfnzIr+C8qakitpvv/54BYjr8R4htXWFqBi842XOE1YDYM5FmXgXMjjda1ZphZ6jdwcOurvqt4F7fM9xk0uwnjXAje75eR28nszWlGCZW902WuHVMoQKHdeISEPx2jvciHfRbAYcUdW5wFQgxaVvLCJd3Xqqi0iBd/CuZmID8Dvgr3q6sV094Cvxumf1N1Q85ObltxNoKSL/5sZ/BfytqB0VkZ+o6qeq+pSLIU9/56qaDUSLiP/C19k9j4/CO05rgb8D3ULbFq8tQyu3jn141daP4BUGANYDvcRrIxKN19lMkbHi1Vbc4obLs+HmSZfH+dUH9ruL/1XAJSVYV4vQMQf+HS9vSn1c8gkfb5fnzVV1Fd5jlvp4j27Aq6U673vpCxIrAJhz0TTgLhHZjFeFXBbH3PIzgDvctMfwnq1vFZHtbrxIqroJ7056PV4Pby+p6uZiFnsBiHJV7wuBND3d29d6vL7StwJvqOonQFtgvav6ngBMVtUTeIWfp0RkC171c1GtyxcCw8j7aGCci/lDvOfoIQuAMa5R2U98+3oMGA687mLPxcu/otznGuFtBU7iVc/ntxKvKjtkA/AcXteoX+A9YvkWr43Ea25dH5O3MDEPr3o8w8X6FfAQXteqW4CNqlpct6r3Ane7fYsrJm1pzMQ7p/K/DjoPSHXbu428x6AwO12MGUAsML2Mx8UvfLyBy4C5bj2bgd+r11c9wFV4bwOY84T1BmiMiSj3eON+Vf3VWazjOWCzqr5cfpFVLSLSEq8Gp00Ett0Ur6vvPpW9bVNxrA2AMSaiVHWTiKwSkWgtw7cARGQjXpuHB8o/OuO0wPL3vGM1AMYYY0wAWRsAY4wxJoCsAGCMMcYEkBUAjDHGmACyAoAxxhgTQFYAMMYYYwLICgDGGGNMAP0fcVTB63m22BsAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hU5WQ-zoWrFp"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}